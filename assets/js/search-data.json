{
  
    
        "post0": {
            "title": "Sentiment Analysis on Amazon Reviews with Python",
            "content": "Introduction . . In this blog, I will guide you through how to perform a sentiment analysis on a text data. . We will be using the Razer_mouse_reviews.csv which I scraped from Amazon using Scrapy in Python to perform our analysis. This is only a portion of the reviews. The data contains the customer reviews for the product &quot;Razer DeathAdder Essential Gaming Mouse&quot; on Amazon and their star ratings. If you want to follow me along, you can download my dataset from here Razer_mouse_reviews.csv. You can also scrape your own dataset of any product you want on Amazon follow this tutorial: scraping amazon reviews use python scrapy. . Data cleaning . Before we start our analysis, we need to clean up our data. . Prerequisites . First, we need to import the necessary packages. Make sure you have installed all the packages below: . numpy | pandas | matplotlib | seaborn | wordcloud | sklearn | . import numpy as np import pandas as pd import seaborn as sns color = sns.color_palette() import matplotlib.pyplot as plt %matplotlib inline from wordcloud import WordCloud, STOPWORDS from sklearn.feature_extraction.text import CountVectorizer from sklearn.linear_model import LogisticRegression from sklearn.model_selection import train_test_split from sklearn.metrics import confusion_matrix,classification_report #Optional, just want to ignore the warning text in the output import warnings from pandas.core.common import SettingWithCopyWarning warnings.simplefilter(action=&quot;ignore&quot;, category=SettingWithCopyWarning) . Read the Dataframe . df1 = pd.read_csv(&quot;Razer_mouse_reviews.csv&quot;) df1.head() . From the table below, we can see the stars are strings instead of integer rating and there are so many white line in front of each comment. . stars comment . 0 | 4.0 out of 5 stars | n n n n n n n n n n n n n For an &quot;el... | . 1 | 2.0 out of 5 stars | n n n n n n n n n n n n n Every time... | . 2 | 2.0 out of 5 stars | n n n n n n n n n n n n n The unit i... | . 3 | 1.0 out of 5 stars | n n n n n n n n n n n n n O.K., Im g... | . 4 | 1.0 out of 5 stars | n n n n n n n n n n n n n The mouse ... | . Clean the Dataframe . Next, we want to replace &quot;1.0 out of 5 stars&quot; to 1, &quot;2.0 out of 5 stars&quot; to 2, &quot;3.0 out of 5 stars&quot; to 3, &quot;4.0 out of 5 stars&quot; to 4, and &quot;5.0 out of 5 stars&quot; to 5 in the stars column. Then use str.strip() to remove all the white lines at the begainning of each comment. The code is like following: . df1 =df1.replace({&quot;1.0 out of 5 stars&quot;: 1, &#39;2.0 out of 5 stars&#39;: 2, &#39;3.0 out of 5 stars&#39;: 3, &#39;4.0 out of 5 stars&#39;: 4, &#39;5.0 out of 5 stars&#39;: 5}) #Remove the new lines at the begainning of the comment df1[&#39;comment&#39;] =df1[&#39;comment&#39;].str.strip() . Good job! Your data should be look like this: . stars comment . 0 | 4 | For an &quot;elite&quot; gaming mouse with impressive fe... | . 1 | 2 | Every time my computer starts or restarts Syna... | . 2 | 2 | The unit is just built cheap. Not the quality ... | . 3 | 1 | O.K., Im going to &quot;throw&quot; this to the air. Nob... | . 4 | 1 | The mouse left click started to break within t... | . Classify Positive and Negative Comments . We will assign comments with rating four and five as 1 which means positive sentiment, rating one and two as -1 which means negative sentiment. Since, we are not interested in rating three which is the neutral sentiment in this analysis, we will drop the comments with rating three. The code below will do the job: . df = df1[df1[&#39;stars&#39;] != 3] #add one column called sentiment contains values 1 and -1 df[&#39;sentiment&#39;] = df[&#39;stars&#39;].apply(lambda rating : +1 if rating &gt; 3 else -1) #add another column sentimentt contains values negative and positive df[&#39;sentimentt&#39;] = df[&#39;sentiment&#39;].replace({-1 : &#39;negative&#39;}) df[&#39;sentimentt&#39;] = df[&#39;sentimentt&#39;].replace({1 : &#39;positive&#39;}) . Your new dataframe will look like this: . stars comment sentiment sentimentt . 0 | 4 | For an &quot;elite&quot; gaming mouse with impressive fe... | 1 | positive | . 1 | 2 | Every time my computer starts or restarts Syna... | -1 | negative | . 2 | 2 | The unit is just built cheap. Not the quality ... | -1 | negative | . 3 | 1 | O.K., Im going to &quot;throw&quot; this to the air. Nob... | -1 | negative | . 4 | 1 | The mouse left click started to break within t... | -1 | negative | . Exploratory Data Analysis . We want explore our dataset to see if there is any interesting dicoveries and findings. Althought, our dataset only has two columns, we can still do a lot of fancy graphs and vidualizations. . Stars Counts . Next, we can viduallize the number of comments in each star rating using seaborn. Make sure you use df1 to do this plot, because we deleted rating 3 in df. . #set plot theme sns.set_theme(style=&quot;darkgrid&quot;) #Specifiy the figure size plt.figure(figsize=(15,8)) ax = sns.countplot(x=&quot;stars&quot;, data = df1, palette=&quot;Blues&quot;) ax.set_title(&quot;Number of Comments in Each Rating &quot;, fontsize=20) ax.set_xlabel(&quot;Star Rating&quot;,fontsize=15) ax.set_ylabel(&quot;Number of Comments&quot;,fontsize=15) plt.show() . The result plot looks like this: . From the plot above, we can see that the majority of the customers rating is positive. On the other hand, there are almost 100 comments are rated one star and two star. Therefore, we can build a model to predict the customers rating based on their comments. We will talk about modeling in the later section. . Sentiment Counts . Now, we can take a closer look. We want to see the number of positive comments and the number of negative comments: . sns.set_theme(style=&quot;darkgrid&quot;) plt.figure(figsize=(15,8)) ax = sns.countplot(x=&quot;sentimentt&quot;, data = df, palette=&quot;coolwarm&quot;) ax.set_title(&quot;Product Sentiment&quot;, fontsize=20) ax.set_xlabel(&quot;Sentiment&quot;,fontsize=15) ax.set_ylabel(&quot;Count&quot;,fontsize=15) plt.show() . Most Frequent Words . Next, we can use the WordCloud to find the most frequent words that appeared in the comments. . # Create stopword list stopwords = set(STOPWORDS) stopwords.update([&quot;mouse&quot;, &quot;Razer&quot;, &quot;gaming&quot;]) #Joint each word in each comment together separate by space textt = &quot; &quot;.join(review for review in df.comment) wordcloud = WordCloud(stopwords=stopwords, background_color = &quot;white&quot;).generate(textt) # Plot the worldclou to show the most frequent words in the image plt.imshow(wordcloud, interpolation=&#39;bilinear&#39;) plt.axis(&quot;off&quot;) plt.show() . The result would be look like this: . From the above image, we can see the word, &quot;great&quot;, &quot;feel&quot;, &quot;use&quot;, &quot;button&quot;, &quot;one&quot;, &quot;software&quot; are the most frequent words. Next, we can also find the most frequent words in the positive and negative comments by split the comments into positive and negative comments. . Most Frequent Words in the Positive Comments . positive = df[df[&#39;sentiment&#39;] == 1] stopwords = set(STOPWORDS) stopwords.update([&quot;mouse&quot;, &quot;Razer&quot;, &quot;gaming&quot;, &quot;use&quot;, &quot;button&quot;]) ## good and great removed because they were included in negative sentiment pos = &quot; &quot;.join(review for review in positive.comment) wordcloud2 = WordCloud(stopwords=stopwords, background_color = &quot;white&quot;).generate(pos) plt.imshow(wordcloud2, interpolation=&#39;bilinear&#39;) plt.axis(&quot;off&quot;) plt.show() . Most Frequent Words in the Nagetive Comments . negative = df[df[&#39;sentiment&#39;] == -1] negative = negative.dropna() neg = &quot; &quot;.join(review for review in negative.comment) wordcloud3 = WordCloud(stopwords=stopwords, background_color = &quot;white&quot;).generate(neg) plt.imshow(wordcloud3, interpolation=&#39;bilinear&#39;) plt.axis(&quot;off&quot;) plt.show() . Data Modeling . Finally, it comes to the most exciting part, data modeling. We want to use logistic regression to predict whether the comments is positive or negative. Before doing so, we have to do some preprocessing. . Preprocessing . Step one: Remove punctuations . You can use the following function to remove puctuations in the comments: . def remove_punctuation(text): final = &quot;&quot;.join(u for u in text if u not in (&quot;?&quot;, &quot;.&quot;, &quot;;&quot;, &quot;:&quot;, &quot;!&quot;,&#39;&quot;&#39;)) return final df[&#39;comment&#39;] = df[&#39;comment&#39;].apply(remove_punctuation) . Your new dataframe will like following table: . stars comment sentiment sentimentt . 0 | 4 | For an elite gaming mouse with impressive feat... | 1 | positive | . 1 | 2 | Every time my computer starts or restarts Syna... | -1 | negative | . 2 | 2 | The unit is just built cheap Not the quality p... | -1 | negative | . 3 | 1 | OK, Im going to throw this to the air Nobody a... | -1 | negative | . 4 | 1 | The mouse left click started to break within t... | -1 | negative | . Step two: Select the Feature for Modeling . In this example, our model only take two teatures. We will use comment to predict sentiment, 1 for positive, -1 for negative. . dfNew = df[[&#39;comment&#39;,&#39;sentiment&#39;]] dfNew.head() . Your selected features are following columns: . comment sentiment . 0 | For an elite gaming mouse with impressive feat... | 1 | . 1 | Every time my computer starts or restarts Syna... | -1 | . 2 | The unit is just built cheap Not the quality p... | -1 | . 3 | OK, Im going to throw this to the air Nobody a... | -1 | . 4 | The mouse left click started to break within t... | -1 | . Step three: Split Train and Test Data . We can randomly split our data using the train_test_split function in the sklearn package, which already imported in the very begining. 80% of our data will be used for training, and 20% will be used for testing. Thre are many other methods to split your data, feel free to use your own way. . train ,test = train_test_split(df,test_size=0.2) . Step four: Vectorize Comments . In this step, we want to Convert the collection of text comments to a matrix of token counts, because the logistic regression algorithm cannot understand text. . We will use a count vectorizer from the Scikit-learn library to transform the text comments into a bag of words model, which will find the unique words in each comment, and count the occurence for each word in each comment. . vectorizer = CountVectorizer(token_pattern=r&#39; b w+ b&#39;) #vectorize both train data and test data train_matrix = vectorizer.fit_transform(train[&#39;comment&#39;]) test_matrix = vectorizer.transform(test[&#39;comment&#39;]) . Logistic Regression . Finally, we can use the Logistic Regression from the Scikit-learn library to fit our trainning data and make predictions using our test data. . lr = LogisticRegression() #Split target and independent variables X_train = train_matrix X_test = test_matrix y_train = train[&#39;sentiment&#39;] y_test = test[&#39;sentiment&#39;] #Fit model on data lr.fit(X_train,y_train) #Make predictions predictions = lr.predict(X_test) . Data Validation . We can test our model accuracy use the confusion matrix, which imported from the sklearn package in the very beginning: . new = np.asarray(y_test) cf_matrix = confusion_matrix(predictions,y_test) #Display our confusion matrix in a heatmap: group_names = [&quot;True Negative&quot;,&quot;False Positive&quot;,&quot;False Negative&quot;,&quot;True Positive&quot;] group_counts = [&quot;{0:0.0f}&quot;.format(value) for value in cf_matrix.flatten()] group_percentages = [&quot;{0:.2%}&quot;.format(value) for value in cf_matrix.flatten()/np.sum(cf_matrix)] labels = [f&quot;{v1} n{v2} n{v3}&quot; for v1, v2, v3 in zip(group_names,group_counts,group_percentages)] labels = np.asarray(labels).reshape(2,2) sns.heatmap(cf_matrix, annot=labels, fmt=&#39;&#39;, cmap=&#39;Blues&#39;) plt.show() . We can also generate a classification report to validate our model accuracy: . print(classification_report(predictions,y_test)) . precision recall f1-score support -1 0.67 0.86 0.75 14 1 0.97 0.91 0.94 70 accuracy 0.90 84 macro avg 0.82 0.89 0.85 84 weighted avg 0.92 0.90 0.91 84 . The overall accuracy of the model on the test data is around 90%, which is pretty good since our dataset is not very large. . Thank you for reading it, I hope this tutorial will help you to understand the basics of the sentiment analysis, WorldCloud, and Logistic Regression. If you have any questions feel free to comment below. Good luck everyone, you are on the right track. .",
            "url": "https://shudi-zhao.github.io/My_Data_Science_Blog/sentiment%20analysis/wordcloud/python/2021/03/14/Sentiment-Analysis.html",
            "relUrl": "/sentiment%20analysis/wordcloud/python/2021/03/14/Sentiment-Analysis.html",
            "date": " • Mar 14, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "My Resume",
            "content": "EDUCATION . Fei Tian College—Middletown, NY Graduating: May 2022 . Bachelor of Science in Data Science Cumulative GPA: 3.91/4.00 . Two-time winner of the Provost’s Outstanding Student Award . | Three-time winner of the Academic Excellence Award . | . Thomas Kelly High School, Chicago, IL June 2018 . Received: The Seal of Biliteracy—English and Mandarin Unweighted GPA: 3.90/4.00 . COURSES . Data Mining, Statistical Computing and Graphics, Database, Cloud Computing and Big Data, Data Structure Algorithms, Linear Regression, Statistical Theory and Method, Business Data Analytics, Microeconomics . PROJECTS . Data Science Project with AWS EC2 and RDS Feb 2020–Aug 2020 . Created a comprehensive system flight booking system with a MySQL database backend to store user login, flight information, and booking data . | Used Python flask and HTML, ran on Amazon EC2, created a user-friendly front-end UI . | . Coronavirus Tracker (Tableau) May 2020–Aug 2020 . Developed an interactive animated Tableau time series analysis dashboard with waterfall charts, line charts, and maps to visualize COVID-19 spreads within the U.S. and globally . | Data manipulation in Excel using advanced formulas (VLOOKUP, Index match, pivot table) . | . Data Analysis in R Project Lead Sep 2019–Dec 2019 . Spearheaded linear regression to forecast SAT score and correlation analysis using the time spent studying and GPA . | Performed data manipulation, data cleaning, normalization, and prescriptive analytics . | . EXPERIENCE . Arc’teryx, Central Valley, NY May 2019–Present . Sales Associate . Results-oriented and increase 30% sales revenue and 35% customer service by leveraging my communication, bilingual literacy and by analyzing foot traffic at the mall  | . Managing large amounts of incoming calls and follow company escalation policy . | Identifying causes of customer problems and complaints with the best solution . | . SKILLS . Advanced programming skills in MySQL, Python, Java, and HTML . | Data analysis and visualization in R ggplot, Tableau, Python Matplotlib and Pandas . | Advanced Proficiency in Microsoft Office, Excel, PowerPoint and Word. . | Strong negotiation, financial analytical skill, qualitative and quantitative research skills . | Project management, public relations, teamwork, detail oriented, professionalism, social skill . | .",
            "url": "https://shudi-zhao.github.io/My_Data_Science_Blog/2020/01/28/My-Resume.html",
            "relUrl": "/2020/01/28/My-Resume.html",
            "date": " • Jan 28, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "Shudi Zhao . Data Science Students | Looking for Data Science Internships | .",
          "url": "https://shudi-zhao.github.io/My_Data_Science_Blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://shudi-zhao.github.io/My_Data_Science_Blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}
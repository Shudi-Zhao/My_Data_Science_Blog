{
  
    
        "post0": {
            "title": "Training a Computer to Blind Taste Wine (Part I)",
            "content": "1. Introduction . Do you like to drink wine? Can you guess the grape type after drinking it? If not, you are in the right place. This project will use some simple supervised machine learning techniques to predict the wine grape variety of which a bottle of wine was made from. In this project, we will use the wine descriptions to find the similarities and differences between wines, then predict the grape variety based on its description. . In fact, all the descriptions were done by wine experts called sommelier. It will take them years to practice in order to master blind tasting. Blind tasting a wine means tasting a wine with no idea of its grape variety, origin, vintage, or any evidence really other than the liquid in front of you. . Actually, this can be done by machine learning and data mining. However the computer cann&#39;t fully replace the sommeliers, we need to provide the machine some based characterizations about the wine like the description to predict the grape variety. . This project is the fisrt part, which I will focus on exploring the similarities between wine descriptions using some simple Natural Language Processing (NLP) techniques. In additon, this whole project is reimplemented based on this Wine Project using spaCy instead of the NLTK package. . This image is from masterclass . 2. Data Cleaning . In this project, I scraped my own data set from Majestic using selenium. If you want to follow me along, you can download my pre-cleaned data from here: my_wine_data_clean.csv. You can also scrape your own data from Majestic, Wine Enthusiast, Bibendum, etc... Another option is to download the big wine dataset from Kaggle published by zackthoutt, it will be a good dataset to start with. We will only use the wine descriptions, and grape varieties in this project. . First, we need to import all the neccessary packages. Make sure you have installed all the libraries in your computer if you want to follow me along. . import pandas as pd import numpy as np from functools import reduce # function to split data into train and test samples from sklearn.model_selection import train_test_split from sklearn.feature_extraction.text import CountVectorizer from sklearn.feature_extraction.text import TfidfVectorizer # packages for visualization import seaborn as sns import matplotlib.pyplot as plt from collections import Counter import spacy nlp = spacy.load(&quot;en_core_web_sm&quot;) plt.style.use(&#39;seaborn-darkgrid&#39;) . Then we will load our wine data. my data have 503 rows and 3 columns which are the wine name, grape variety, and description. Below you can see the first five rows of my dataframe. In this project we are mainly interested in the grape_variety and description column. . a0 = pd.read_csv(&quot;my_wine_data_clean.csv&quot;) a0.head() . Unnamed: 0 name grape_variety description . 0 | 0 | Edouard Delaunay &#39;Septembre&#39; Chardonnay 2019, ... | Chardonnay | One half of the Abbotts &amp; Delaunay powerhouse,... | . 1 | 1 | The Ned Waihopai River Sauvignon Blanc 2020 Ma... | Sauvignon Blanc | Our bestselling white wine. Winemaker Brent Ma... | . 2 | 2 | The King&#39;s Favour Sauvignon Blanc 2019/20 Marl... | Sauvignon Blanc | Brent Marris is winemaking royalty. And it tur... | . 3 | 3 | Château Livran 2014, Médoc | Cabernet Sauvignon | Château Livran once belonged to both Edward I ... | . 4 | 4 | Cave de Lugny &#39;Reserve&#39; Mâcon-Chardonnay 2019 | Chardonnay | The small village of Mâcon-Chardonnay isn’t co... | . Next, we want to find out what are the grape varieties in our dataframe, and how many rows for each type. . df_val_counts = pd.DataFrame(a0[&#39;grape_variety&#39;].value_counts()) df_val_counts.head(10) . grape_variety . Chardonnay | 219 | . Pinot Noir | 101 | . Cabernet Sauvignon | 92 | . Sauvignon Blanc | 91 | . From the table above, we can see there are four grape types in our dataframe, and the number of appearence is accpeted. Therefore, we are going to prediction whether the wine is made of Chardonnay, Pinot Noir, Cabernet Sauvignon, or Sauvignon Blanc with their descriptions. . First, we need to split our data into test and train set: . combined_features = [&#39;description&#39;, &#39;grape_variety&#39;] target = &#39;grape_variety&#39; X_train, X_test, y_train, y_test = train_test_split(a0[combined_features], data_input[target], test_size=0.30, random_state=42) . The following bar chart shows the number of descriptions in each grape variety: . #set plot theme and font size sns.set(style=&quot;darkgrid&quot;, font_scale=1.2) #Specifiy the figure size plt.figure(figsize=(10,6)) ax = sns.countplot(x=&quot;grape_variety&quot;, data = X_train, palette=&quot;pastel&quot;, order = X_train[&#39;grape_variety&#39;].value_counts().index) ax.set_title(&quot;Number of Descriptions in Grape Variety&quot;, fontsize=20) ax.set_xlabel(&#39;Count&#39;) ax.set_ylabel(&#39;Grape Varieties&#39;) plt.show() . Then, let&#39;s combine all the descriptions for each grape variety into one row, so we can find some patterns to distinguish them. . grouped = X_train[[&#39;grape_variety&#39;, &#39;description&#39;]].groupby([&#39;grape_variety&#39;]).agg( {&#39;description&#39;: lambda z: reduce(lambda x,y: &#39;&#39;.join(x+y), z)} ) grouped[&quot;description&quot;] = grouped[&quot;description&quot;].str.lower() . Your will get the following dataframe: . description . grape_variety . Cabernet Sauvignon | château livran once belonged to both edward i ... | . Chardonnay | produced exclusively from top quality chardonn... | . Pinot Noir | you might think california is too hot for the ... | . Sauvignon Blanc | this dessert wine from marisco vineyards&#39; king... | . 2.1 Word Tokenization . We will need to define a funtion to tokenize our descriptions, so we are able to compare the words used in each grape variety. In the function word_count_df below, the parameter df is the input dataframe, src_col is the column name in df that you want to tokenize. The function will return you a pandas dataframe with three columns, grape, token, and count. You can also modify the column names in the parameter out_col. . Basically, we use a spaCy model to tokenize descriptions for each grape variety, then count the number of appearence for each unique words in each description. Finally return the word counts to a pandas dataframe. . def word_count_df(df, src_col, out_col=(&#39;grape&#39;, &#39;token&#39;, &#39;count&#39;)): dfp = pd.DataFrame() for i in df.index: doc = nlp(df[df.index == i][src_col].values[0]) counts = Counter() for token in doc: counts[token.text] += 1 dftmp = pd.DataFrame(dict(zip(out_col, ([i]*len(counts.keys()), list(counts.keys()), list(counts.values()))))) dfp=pd.concat([dfp, dftmp], ignore_index=True) return(dfp) . Let&#39;s call our word_count_df function: . tkn_count = word_count_df(grouped, &#39;description&#39;, out_col=(&#39;grape&#39;, &#39;token&#39;, &#39;count&#39;)) . Your tkn_count will look like the following table: . grape token count . 0 | Cabernet Sauvignon | château | 14 | . 1 | Cabernet Sauvignon | livran | 2 | . 2 | Cabernet Sauvignon | once | 3 | . 3 | Cabernet Sauvignon | belonged | 1 | . 4 | Cabernet Sauvignon | to | 42 | . ... | ... | ... | ... | . 6274 | Sauvignon Blanc | enchanting | 1 | . 6275 | Sauvignon Blanc | soaked | 1 | . 6276 | Sauvignon Blanc | sun | 1 | . 6277 | Sauvignon Blanc | gooseberry.awatere | 1 | . 6278 | Sauvignon Blanc | cheeses | 1 | . 6279 rows × 3 columns . Now, let&#39;s define a function to plot a barplot matrix to show the most frequent tokens in each grape variety: . def barplot_wordcounts(df, limit = 10): # Create subsets for each grap type chardonnay = df[df[&#39;grape&#39;] == &quot;Chardonnay&quot;].sort_values(by=[&#39;count&#39;], ascending=False).head(limit).sort_values(by=[&#39;count&#39;]) sauvignon_blanc = df[df[&#39;grape&#39;] == &quot;Sauvignon Blanc&quot;].sort_values(by=[&#39;count&#39;], ascending=False).head(limit).sort_values(by=[&#39;count&#39;]) pinot_noir= df[df[&#39;grape&#39;] == &quot;Pinot Noir&quot;].sort_values(by=[&#39;count&#39;], ascending=False).head(limit).sort_values(by=[&#39;count&#39;]) syrah = df[df[&#39;grape&#39;] == &quot;Cabernet Sauvignon&quot;].sort_values(by=[&#39;count&#39;], ascending=False).head(limit).sort_values(by=[&#39;count&#39;]) #build the plot fig, ax = plt.subplots(2, 2, figsize=(15, 8)) ax[0, 0].barh(chardonnay[&#39;token&#39;], chardonnay[&#39;count&#39;], color= &quot;gold&quot;) ax[0, 0].set_ylabel(&quot;Tokens&quot;) ax[0, 0].set_title(&quot;Chardonnay&quot;) ax[0, 1].barh(sauvignon_blanc[&#39;token&#39;], sauvignon_blanc[&#39;count&#39;], color= &quot;deepskyblue&quot;) ax[0, 1].set_title(&quot;Sauvignon Blanc&quot;) ax[1, 0].barh(pinot_noir[&#39;token&#39;], pinot_noir[&#39;count&#39;], color= &quot;violet&quot;) ax[1, 0].set_ylabel(&quot;Tokens&quot;) ax[1, 0].set_xlabel(&quot;Count&quot;) ax[1, 0].set_title(&quot;Pinot Noir&quot;) ax[1, 1].barh(syrah[&#39;token&#39;], syrah[&#39;count&#39;], color= &quot;limegreen&quot;) ax[1, 1].set_xlabel(&quot;Count&quot;) ax[1, 1].set_title(&quot;Cabernet Sauvignon&quot;) plt.tight_layout()#Get rid of overlaps plt.show() . Below is the top 15 most frequent words used in Chardonnay, Sauvignon Blanc, Pinot Noir, and Cabernet Sauvignon descriptions. From the plot, we can see there are a lot of punctuations and common words like the, and, of, a, in, is, it, and to etc.. Therefore, we cann&#39;t really tell the difference between these four types of grapes. . barplot_wordcounts(tkn_count, limit = 15) . 2.2 Filtering Noise . Next, we need to filter out the noises from our token list, so we can see the difference between each grape type clearly. SpaCy have a list of build-in stop words, we can also add some customized stop words to it like what I did in the follow: . customize_stop_words = [&quot;wine&quot;,&quot;wines&quot;,&quot;fruit&quot;,&quot;fruits&quot;, &quot;flavour&quot;,&#39;flavours&#39;, &#39;aromas&#39;, &#39;palate&#39;, &#39;chardonnay&#39;,&#39;notes&#39;, &#39;note&#39;,&#39;sauvignon&#39;, &#39;blanc&#39;, &#39;de&#39;, &#39;pinot&#39;, &#39;noir&#39;, &#39;cabernet&#39;, &#39;best&#39;] for w in customize_stop_words: nlp.vocab[w].is_stop = True . Then, we can just simply modify the word_count_df function that we defined previously to remove the stop words and punctuations. token.is_alpha will check whether the word is alphabatic characters or not, so this will remove the punctuations. token.is_stop will check whether the word is stop words or not, we can only keep the words that are not stop words. . def word_count_df1(df, src_col, out_col=(&#39;grape&#39;, &#39;token&#39;, &#39;count&#39;)): dfp = pd.DataFrame() for i in df.index: doc = nlp(df[df.index == i][src_col].values[0]) counts = Counter() for token in doc: #remove non-alphabatic characters and stop words if token.is_alpha == True and token.is_stop == False: counts[token.text] += 1 dftmp = pd.DataFrame(dict(zip(out_col, ([i]*len(counts.keys()), list(counts.keys()), list(counts.values()))))) dfp=pd.concat([dfp, dftmp], ignore_index=True) return(dfp) . After filtered out the noises, our dataframe will look like the folloing table. Compare to the previous table, we removed 1347 rows. . tkn_count1 = word_count_df1(grouped, &#39;description&#39;, out_col=(&#39;grape&#39;, &#39;token&#39;, &#39;count&#39;)) tkn_count1 . grape token count . 0 | Cabernet Sauvignon | château | 14 | . 1 | Cabernet Sauvignon | livran | 2 | . 2 | Cabernet Sauvignon | belonged | 1 | . 3 | Cabernet Sauvignon | edward | 1 | . 4 | Cabernet Sauvignon | pope | 1 | . ... | ... | ... | ... | . 4927 | Sauvignon Blanc | sounds | 1 | . 4928 | Sauvignon Blanc | enchanting | 1 | . 4929 | Sauvignon Blanc | soaked | 1 | . 4930 | Sauvignon Blanc | sun | 1 | . 4931 | Sauvignon Blanc | cheeses | 1 | . 4932 rows × 3 columns . After filtering, some of the characteristics start to apppear in the most frequent words. From the table below, we can actually differentiate the varieties by these most frequent words. . barplot_wordcounts(tkn_count1, limit = 10) . 2.3 Lemmatization . In the previous step, we removed some basic noise from the words token counts, but that is not enough. We can see grapes and grape, cherry and cherries. In fact, they are the same but one is singular noun another one is plural noun. . The good news is that we can simply lemmatize the tokens using spaCy. token.lemma is the lemmatized word. For example, grapes will become grape. Finally, we can simply add the token.lemma into the word_count_df1 function to limmatized the tokens. . def word_count_df2(df, src_col, out_col=(&#39;grape&#39;, &#39;token&#39;, &#39;count&#39;)): dfp = pd.DataFrame() for i in df.index: doc = nlp(df[df.index == i][src_col].values[0]) counts = Counter() for token in doc: if token.is_alpha == True and token.is_stop == False: #remove non-alphabatic characters and stop words counts[token.lemma_] += 1 # Lemmatization dftmp = pd.DataFrame(dict(zip(out_col, ([i]*len(counts.keys()), list(counts.keys()), list(counts.values()))))) dfp=pd.concat([dfp, dftmp], ignore_index=True) return(dfp) . Below is the token count table after lemmatization, compare to the previous table we have removed 503 rows. . tkn_count2 = word_count_df2(grouped, &#39;description&#39;, out_col=(&#39;grape&#39;, &#39;token&#39;, &#39;count&#39;)) tkn_count2 . grape token count . 0 | Cabernet Sauvignon | château | 14 | . 1 | Cabernet Sauvignon | livran | 2 | . 2 | Cabernet Sauvignon | belong | 1 | . 3 | Cabernet Sauvignon | edward | 1 | . 4 | Cabernet Sauvignon | pope | 1 | . ... | ... | ... | ... | . 4424 | Sauvignon Blanc | dusky | 1 | . 4425 | Sauvignon Blanc | sound | 1 | . 4426 | Sauvignon Blanc | enchant | 1 | . 4427 | Sauvignon Blanc | soak | 1 | . 4428 | Sauvignon Blanc | sun | 1 | . 4429 rows × 3 columns . After lemmatization, we can see the word &#39;grapes&#39; become &#39;grape&#39; and their count increased, because it adds up the token count for both grape and grapes. . barplot_wordcounts(tkn_count2, limit = 10) . 3. Comparison . 3.1 Common Words . In the previous bar chart, we can see there are some words that are overlaped between the grapes. Next, we want to find the common words in the descriptions of each grape variety. Common words are the words that are not giving much information about the individual grape variety. After finding the common words we can check the correlation of their frequency between each grape variety. . dfs = [] #build a list of four objects of df for each graph. for gr in varieties: tmp = tkn_count2[tkn_count2.grape == gr] tmp = tmp.add_suffix(&#39;_&#39;+gr) tmp.columns=tmp.columns.str.replace(&#39;token_&#39;+gr,&#39;token&#39;) dfs.append(tmp) # merge each grape variety with each other on tokens df_final = reduce(lambda left,right: pd.merge(left,right,on=&#39;token&#39;, how=&#39;outer&#39;), dfs) . df_common = df_final.dropna() #Select the columns we need (the count column and token column) cols = df_common.columns.str.contains(&#39;count&#39;) | df_common.columns.str.contains(&#39;token&#39;) df_common = df_common[df_common.columns[cols]] df_common . token count_Chardonnay count_Pinot Noir count_Cabernet Sauvignon count_Sauvignon Blanc . 0 | produce | 21.0 | 9.0 | 5.0 | 4.0 | . 2 | quality | 22.0 | 10.0 | 3.0 | 5.0 | . 3 | grape | 49.0 | 25.0 | 10.0 | 18.0 | . 4 | grow | 9.0 | 5.0 | 1.0 | 3.0 | . 8 | vineyard | 45.0 | 26.0 | 4.0 | 12.0 | . ... | ... | ... | ... | ... | ... | . 1367 | boast | 1.0 | 2.0 | 2.0 | 1.0 | . 1387 | heart | 2.0 | 1.0 | 3.0 | 1.0 | . 1611 | roasted | 1.0 | 4.0 | 1.0 | 1.0 | . 1644 | standard | 1.0 | 2.0 | 1.0 | 1.0 | . 1658 | complement | 1.0 | 1.0 | 1.0 | 1.0 | . 194 rows × 5 columns . Below is a correlation heatmap for the frequency of the common words between each grape variety. . sns.heatmap(df_common.corr() , xticklabels=df_common.corr().columns, yticklabels=df_common.corr().columns, annot=True, cmap = &#39;Blues&#39;, vmin=0, vmax=1) plt.show() . Actually, We cannot simply use the word frequency for each grape variety because the sample size of the grape descriptions are not the same. Therefore, we have to normalize the word count within each grape variety using the MinMaxScaler from the sklearn package. MinMaxScaler will generate outputs numbers between 0 and 1. . from sklearn.preprocessing import MinMaxScaler scaler = MinMaxScaler() df_norm = df_common.copy() sort_col = df_norm.columns.str.contains(&#39;count&#39;) df_norm[df_norm.columns[sort_col]] = scaler.fit_transform(df_norm[df_norm.columns[sort_col]]) df_norm[&#39;freq&#39;] = df_norm[df_norm.columns[sort_col]].sum(axis=1) # visualize the 10 most frequent word within grape verieties dfplot = df_norm.sort_values(by = [&#39;freq&#39;], ascending = False).head(15) dfplot . token count_Chardonnay count_Pinot Noir count_Cabernet Sauvignon count_Sauvignon Blanc freq . 3 | grape | 0.923077 | 0.500000 | 0.473684 | 0.68 | 2.576761 | . 171 | year | 0.711538 | 0.375000 | 0.684211 | 0.52 | 2.290749 | . 50 | oak | 0.923077 | 0.458333 | 0.736842 | 0.16 | 2.278252 | . 205 | fresh | 0.711538 | 0.291667 | 0.263158 | 1.00 | 2.266363 | . 164 | red | 0.115385 | 1.000000 | 1.000000 | 0.00 | 2.115385 | . 183 | fine | 0.596154 | 0.229167 | 0.947368 | 0.24 | 2.012689 | . 8 | vineyard | 0.846154 | 0.520833 | 0.157895 | 0.44 | 1.964882 | . 37 | rich | 0.769231 | 0.083333 | 0.684211 | 0.36 | 1.896775 | . 40 | white | 1.000000 | 0.041667 | 0.052632 | 0.76 | 1.854298 | . 131 | world | 0.500000 | 0.145833 | 0.631579 | 0.36 | 1.637412 | . 228 | expect | 0.423077 | 0.270833 | 0.473684 | 0.36 | 1.527594 | . 197 | ripe | 0.403846 | 0.104167 | 0.894737 | 0.12 | 1.522750 | . 9 | great | 0.538462 | 0.291667 | 0.631579 | 0.04 | 1.501707 | . 161 | vintage | 0.557692 | 0.208333 | 0.526316 | 0.12 | 1.412341 | . 65 | finish | 0.230769 | 0.145833 | 0.578947 | 0.44 | 1.395550 | . Then I will plot the proportion of each grape for the top 15 most frequent conmmon words . from matplotlib import cm import matplotlib cmap = cm.get_cmap(&#39;cool&#39;) fsize = 12 dfplot.index = dfplot[&#39;token&#39;] sort_col = dfplot.columns[dfplot.columns.str.contains(&#39;count&#39;)] # creating the proportion columns llabel = [] for col in sort_col: llabel.append(col.replace(&#39;count_&#39;,&#39;&#39;)) dfplot[col] = dfplot[col] / dfplot[&#39;freq&#39;] # plotting ax = dfplot.loc[list(reversed(dfplot.index[:15])), sort_col].plot(kind=&#39;barh&#39;, stacked=True, cmap=cmap, figsize=(10, 6), fontsize=fsize) ax.legend(llabel,loc=&#39;best&#39;, bbox_to_anchor=(1., 1.), fontsize=fsize) ax.set_facecolor(&#39;w&#39;) ax.set_frame_on(False) ax.set_ylabel(ax.get_ylabel(), fontsize=fsize) plt.show() . 3.2 Disjoint Words . Now, let&#39;s see what are the disjoint words or the words that are unique in each grape variety. . df_unique = pd.DataFrame() for gr in varieties: cond = ~(df_final.columns.str.contains(gr) | df_final.columns.str.contains(&#39;token&#39;)) ind = df_final[df_final.columns[cond]].isna().all(1) # find unique words in each column tmp = df_final.loc[ind, ~cond] tmp.columns = tmp.columns.str.replace(&#39;_&#39;+gr, &#39;&#39;) df_unique = pd.concat([df_unique, tmp], ignore_index=True, sort = True) df_unique . count grape token . 0 | 1.0 | Chardonnay | comte | . 1 | 1.0 | Chardonnay | patient | . 2 | 5.0 | Chardonnay | establish | . 3 | 2.0 | Chardonnay | inland | . 4 | 3.0 | Chardonnay | sumptuous | . ... | ... | ... | ... | . 1705 | 1.0 | Sauvignon Blanc | flow | . 1706 | 1.0 | Sauvignon Blanc | dusky | . 1707 | 1.0 | Sauvignon Blanc | sound | . 1708 | 1.0 | Sauvignon Blanc | enchant | . 1709 | 1.0 | Sauvignon Blanc | soak | . 1710 rows × 3 columns . From the plot below, we can see some of the grape characteristics, but these words cannot be the best describers, because we are only showing the common and disjoint features so far. There may be features in the partially disjoint words. . barplot_wordcounts(df_unique, limit = 10) . 4. Term Frequency And The Count Vectorizer . Actually, we have packages to do the word count. We can use the CountVectorizer from Scikit-learn to count the term frequency of a document. However, before we call the funtion we need to preprosess the descriptions. We will define a remove_stop_words_and_lemmatization() funtion to remove stop words, punctuations, and lemmatize the words in the descriptions. . def remove_stop_words_and_lemmatization(text): my_doc = nlp(text) # Create list of word tokens token_list = [] for token in my_doc: token_list.append(token.lemma_) # Create list of word tokens after removing stopwords filtered_sentence =[] for word in token_list: lexeme = nlp.vocab[word] if lexeme.is_stop == False and lexeme.is_alpha == True: #filter out stop word and non-alpha words filtered_sentence.append(word) s = &#39; &#39; text_final = s.join(filtered_sentence) return text.replace(text, text_final) . We will apply the remove_stop_words_and_lemmatization() function to the description column in the dataframe called grouped. . grouped[&#39;description&#39;] = grouped[&#39;description&#39;].apply(remove_stop_words_and_lemmatization) . Then, we can call the CountVectorizer and fit it on the wine descriptions to create a word counts. We can use the transform method to obtain a bag of words 4 x K sparse matrix. . count_vec = CountVectorizer(analyzer=&#39;word&#39;, ngram_range=(1, 1), max_df=1.0, min_df=1, max_features=None) . count_train = count_vec.fit(grouped.loc[:,&#39;description&#39;]) # based on the fitted vocabulary transform the document into vector representation with counts bag_of_words = count_vec.transform(grouped.loc[:,&#39;description&#39;]) . Now, let&#39;s take a look at what the bag of words look like. We need to convert the sparse matrix to ndarray, so we can visualize it. In the following array, each row means each grape type, each number in the row coorrespond to the term frequency of a word in that row. . bag_of_words.toarray() . array([[0, 0, 0, ..., 0, 0, 0], [1, 0, 1, ..., 3, 4, 1], [1, 2, 0, ..., 0, 0, 0], [0, 0, 0, ..., 5, 3, 1]]) . Next, we will convert the bag of words to a pandas dataframe: . a=pd.DataFrame(bag_of_words.toarray(), columns=sorted(count_vec.vocabulary_)) # take its transpose a=a.T # rename column names a.columns=grouped.index a . grape_variety Cabernet Sauvignon Chardonnay Pinot Noir Sauvignon Blanc . aarde | 0 | 1 | 1 | 0 | . abbott | 0 | 0 | 2 | 0 | . ability | 0 | 1 | 0 | 0 | . abound | 0 | 0 | 1 | 0 | . absolute | 0 | 1 | 0 | 0 | . ... | ... | ... | ... | ... | . zealand | 1 | 4 | 7 | 6 | . zesti | 0 | 0 | 1 | 0 | . zesty | 0 | 3 | 0 | 5 | . zingy | 0 | 4 | 0 | 3 | . zippy | 0 | 1 | 0 | 1 | . 2691 rows × 4 columns . Calculate the correlation heatmap of the term frequency for each grape variety: . a_count = a[a.columns].corr() sns.heatmap(a_count, xticklabels=a_count.columns, yticklabels=a_count.columns, annot=True, cmap = &#39;Blues&#39;, vmin=0, vmax=1) plt.show() . 5. Inverse Document Frequency And The TF-IDF Vectorizer . Another method to do the word vectorizer is the term frequency combined with inverse document frequency. If you are not familiar with TF-IDF, you can reference this article: TF-IDF. Basically we can follow the same step as we did for the CountVectorizer. . tfid_vec = TfidfVectorizer(analyzer=&#39;word&#39;, ngram_range=(1, 1), max_df=1.0, min_df=1, max_features=None) tfidf_train = tfid_vec.fit(grouped.loc[:,&#39;description&#39;]) bag_of_words = tfid_vec.transform(grouped.loc[:,&#39;description&#39;]) . To help us to get a better understanding on what actually the idf doing, we convert our vocabulary with the correspond idf factors in to the following data frame. . word_idf = pd.DataFrame( { &#39;token&#39;: list(sorted(tfid_vec.vocabulary_)), &#39;idf&#39;: tfid_vec.idf_} ) word_idf.head(10) . token idf . 0 | aarde | 1.510826 | . 1 | abbott | 1.916291 | . 2 | ability | 1.916291 | . 3 | abound | 1.916291 | . 4 | absolute | 1.916291 | . 5 | absolutely | 1.510826 | . 6 | abundance | 1.916291 | . 7 | abundant | 1.916291 | . 8 | acacia | 1.916291 | . 9 | access | 1.916291 | . As you can see, there are only for unique values, becuae we only have four rows of data or four grape varieties. . idfs = pd.unique(word_idf[&#39;idf&#39;].values) idfs . array([1.51082562, 1.91629073, 1. , 1.22314355]) . Then, lets print out the proportion of words that contained in 1 document, 2 documents, 3 documents, and 4 documents. . print(&#39;Ratio of words contained in 1 document: %.3f pct&#39; % (word_idf[word_idf[&#39;idf&#39;]==idfs[0]].shape[0]/word_idf.shape[0]*100)) # number of words contained in 1 document print(&#39;Ratio of words contained in 2 documents: %.3f pct&#39; % (word_idf[word_idf[&#39;idf&#39;]==idfs[1]].shape[0]/word_idf.shape[0]*100)) # number of words contained in 2 documents print(&#39;Ratio of words contained in 3 documents: %.3f pct&#39; % (word_idf[word_idf[&#39;idf&#39;]==idfs[2]].shape[0]/word_idf.shape[0]*100)) # number of words contained in 3 documents print(&#39;Ratio of words contained in 4 documents: %.3f pct&#39; % (word_idf[word_idf[&#39;idf&#39;]==idfs[3]].shape[0]/word_idf.shape[0]*100)) # number of words contained in 4 documents . Ratio of words contained in 1 document: 20.810 pct Ratio of words contained in 2 documents: 62.430 pct Ratio of words contained in 3 documents: 7.061 pct Ratio of words contained in 4 documents: 9.699 pct . Here, we will obtain the bag of word data frame using the tfidf method: . a=pd.DataFrame(bag_of_words.toarray(), columns=sorted(tfid_vec.vocabulary_)) a=a.T a.columns=grouped.index atfidf = a.corr() a . grape_variety Cabernet Sauvignon Chardonnay Pinot Noir Sauvignon Blanc . aarde | 0.000000 | 0.004938 | 0.010125 | 0.000000 | . abbott | 0.000000 | 0.000000 | 0.025683 | 0.000000 | . ability | 0.000000 | 0.006263 | 0.000000 | 0.000000 | . abound | 0.000000 | 0.000000 | 0.012842 | 0.000000 | . absolute | 0.000000 | 0.006263 | 0.000000 | 0.000000 | . ... | ... | ... | ... | ... | . zealand | 0.008606 | 0.013074 | 0.046909 | 0.045126 | . zesti | 0.000000 | 0.000000 | 0.012842 | 0.000000 | . zesty | 0.000000 | 0.014814 | 0.000000 | 0.056814 | . zingy | 0.000000 | 0.019752 | 0.000000 | 0.034089 | . zippy | 0.000000 | 0.004938 | 0.000000 | 0.011363 | . 2691 rows × 4 columns . Then, let us again compare the correlation between different grape varieties using this tf-idf results. . sns.heatmap(atfidf, xticklabels=atfidf.columns, yticklabels=atfidf.columns, annot=True, cmap = &#39;Blues&#39;, vmin=0, vmax=1) plt.show() . Finally, let us compare the correlation between the common words of different grape varieties using tf-idf: . acommon = a[a.index.isin(list(word_idf[word_idf[&#39;idf&#39;]==idfs[3]][&#39;token&#39;]))].corr() sns.heatmap(acommon, xticklabels=acommon.columns, yticklabels=acommon.columns, annot=True, cmap = &#39;Blues&#39;, vmin=0, vmax=1) plt.show() . Conclusion . Overall, we performed some text analysis using spaCy to find characteristic features of grape varieties from the wine description. We also obtained the bag-of-word using the CountVectorizer and TfidfVectorizer classes from the Scikit-learn package. . We can clearly see that TfidfVectorizer performs better, because it we give us a lower correlation between grapes compare to CountVectorizer. Therefore, we will use TfidfVectorizer in the next part to predict grape varieties using Random Forest Classifier .",
            "url": "https://shudi-zhao.github.io/My_Data_Science_Blog/spacy/python/machine%20learning/data%20mining/nlp/randomforest/2021/05/07/Wine-Sommelier-Analysi-Part1.html",
            "relUrl": "/spacy/python/machine%20learning/data%20mining/nlp/randomforest/2021/05/07/Wine-Sommelier-Analysi-Part1.html",
            "date": " • May 7, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Stock Price Prediction using Python",
            "content": "Introduction . Nowadays, Data Science become a very powerful tool on almost every industries since we will generate a huge amount of data everyday. Finance and investment is a very hot field for a lot of data scientists, because they can predict or forecast the future price of the stock. In this project, I will guide you throught how to predict the stock price of the most active stock based on the historical data. I hope this project can give you some inspiration on stock predictions. This project is reimplemented based this wonderful tutorial: Predicting Stock Prices with Python . Before we start, let me give you some basic ideal. First, we are going to use selenium for web scraping. We need to get the name of the most active stock on yahoo finance. Selenium is a hot tool not only in web scraping but also in automated web application test. If you haven&#39;t use selenium before, I recommend you to watch this fantanstic youtube selenium tutorial to set up selenium. And use this selenium python tutorial as more detailed reference. Then, we will get the historical data of that most active stock. Next, we will perform some prediction tasks using machine simple learning models. Finally, we can send our predictions to our clients by email. . The image is from this website:goldennest.sg . Prerequisites . First, we need to import the necessary packages. Make sure you have installed all the packages below: . numpy | pandas | matplotlib | sklearn | iexfinance | selenium | . import numpy as np from datetime import datetime import smtplib from selenium import webdriver import os import pandas as pd #For Prediction from sklearn.linear_model import LinearRegression from sklearn import preprocessing#,cross_validation this is no longer aviliable from sklearn.model_selection import train_test_split #use this instead #For Stock Data from iexfinance.stocks import get_historical_data from iexfinance.refdata import get_symbols import matplotlib.pyplot as plt import copy . Get Stock Name . First, we need to create our chrome drive then use driver.get(url) navigate to our desired webpage: https://finance.yahoo.com/most-active which will display the top 25 most active stocks in this page. If you are interested in other stocks you can change this link to the URL you want. Inside webdriver.Chrome() you will need to type your chromedriver path. . driver = webdriver.Chrome( &#39;Type the directory of your chromedriver here&#39;) url = &quot;https://finance.yahoo.com/most-active&quot; driver.get(url) . Next, we want to find the xpath of the most active stock name. You can follow the following steps to get the xpath: . First, go to your desired webpage and inspect the element of that webpage. . Click the &quot;Select an Element&quot; button: . . Click on the first ticker: . . Next, copy the xpath following the instruction below: . . After you find the xpath, you can get the element use the code below: . ticker = driver.find_element_by_xpath( &#39;//*[@id=&quot;scr-res-table&quot;]/div[1]/table/tbody/tr[1]/td[1]/a&#39;) . Last, we want to get the stock name by calling ticker.text, and make a deep copy, because we will lose the stock name after we call driver.quit(). . stock = copy.deepcopy(ticker.text) driver.quit() . Finally, we successfully scraped the stock name of the most active stock using selenium. Below is the name we got from the yahoo finance: . stock . &#39;SNDL&#39; . Get Historical Data . Now, we can get the historical data for the most active stock. Inside the get_historical_data(), you will need to set the start and end date, the output format (we will use pandas in this project). . For the token, you will need to visit iexcloud to create an account to get your API token. You can choose the free version, but it only offers you a very limited access. . Last, we will save the historical data into a csv file. . start = datetime(2020, 3, 1) end = datetime(2021, 3, 1) #Download Historical stock data df = get_historical_data(stock, start, end, output_format=&#39;pandas&#39;, token=&quot;pk_422a359c341b427ea05864740c233fe3&quot;) csv_name = ( stock + &#39;.csv&#39;) df.to_csv(csv_name) . Your dataframe should look like this: . Unnamed: 0 close high low open symbol volume id key subkey ... uLow uVolume fOpen fClose fHigh fLow fVolume label change changePercent . 0 | 2020-03-02 | 1.48 | 1.540 | 1.40 | 1.44 | SNDL | 1056600 | HISTORICAL_PRICES | SNDL | NaN | ... | 1.40 | 1056600 | 1.44 | 1.48 | 1.540 | 1.40 | 1056600 | Mar 2, 20 | 0.06 | 0.0423 | . 1 | 2020-03-03 | 1.48 | 1.500 | 1.35 | 1.45 | SNDL | 945902 | HISTORICAL_PRICES | SNDL | NaN | ... | 1.35 | 945902 | 1.45 | 1.48 | 1.500 | 1.35 | 945902 | Mar 3, 20 | 0.00 | 0.0000 | . 2 | 2020-03-04 | 1.65 | 1.700 | 1.47 | 1.50 | SNDL | 1522873 | HISTORICAL_PRICES | SNDL | NaN | ... | 1.47 | 1522873 | 1.50 | 1.65 | 1.700 | 1.47 | 1522873 | Mar 4, 20 | 0.17 | 0.1149 | . 3 | 2020-03-05 | 1.45 | 1.650 | 1.44 | 1.60 | SNDL | 673430 | HISTORICAL_PRICES | SNDL | NaN | ... | 1.44 | 673430 | 1.60 | 1.45 | 1.650 | 1.44 | 673430 | Mar 5, 20 | -0.20 | -0.1212 | . 4 | 2020-03-06 | 1.33 | 1.475 | 1.33 | 1.45 | SNDL | 494977 | HISTORICAL_PRICES | SNDL | NaN | ... | 1.33 | 494977 | 1.45 | 1.33 | 1.475 | 1.33 | 494977 | Mar 6, 20 | -0.12 | -0.0828 | . 5 rows × 26 columns . Modeling . Before modeling, we need to clean our data a little bit. First, we will select the useful features, because there are too many columns in this dataframe. Then, add the prediction column. . #read the data data = pd.read_csv(csv_name) #feature selection df = data[[&#39;close&#39;, &#39;high&#39;, &#39;low&#39;, &#39;open&#39;,&#39;volume&#39;, &#39;change&#39;]] #add a prediction column (eg: today&#39;s prediction is the close price of tomorrow) df[&#39;prediction&#39;] = df[&#39;close&#39;].shift(-1) #drop the last row, because the value in the prediction column is nan df.dropna(inplace=True) . Your new data frame will look like this: . close high low open volume change prediction . 0 | 1.48 | 1.540 | 1.40 | 1.440 | 1056600 | 0.06 | 1.48 | . 1 | 1.48 | 1.500 | 1.35 | 1.450 | 945902 | 0.00 | 1.65 | . 2 | 1.65 | 1.700 | 1.47 | 1.500 | 1522873 | 0.17 | 1.45 | . 3 | 1.45 | 1.650 | 1.44 | 1.600 | 673430 | -0.20 | 1.33 | . 4 | 1.33 | 1.475 | 1.33 | 1.450 | 494977 | -0.12 | 1.22 | . ... | ... | ... | ... | ... | ... | ... | ... | . 246 | 1.43 | 1.600 | 1.40 | 1.425 | 255266388 | -0.10 | 1.26 | . 247 | 1.26 | 1.330 | 1.10 | 1.290 | 397249358 | -0.17 | 1.45 | . 248 | 1.45 | 1.470 | 1.28 | 1.320 | 433296256 | 0.19 | 1.37 | . 249 | 1.37 | 1.640 | 1.36 | 1.540 | 391487356 | -0.08 | 1.33 | . 250 | 1.33 | 1.490 | 1.31 | 1.390 | 255416545 | -0.04 | 1.35 | . 251 rows × 7 columns . Next, we will built our regression model, a detailed explaination is commented below: . #X is the predictor variable, Y is the target variable X = np.array(df.drop([&#39;prediction&#39;], 1)) Y = np.array(df[&#39;prediction&#39;]) #Nomalize our predictor variables X = preprocessing.scale(X) #the last row in the predictor variable X_prediction = X[-1:] #the last row in the target variable Y_ans = Y[-1:] #Delete the last row in X and Y, because we don&#39;t want it to be in the train data. X = X[:-1] Y = Y[:-1] #Split our data into train and test data X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2) #Performing the Regression on the training data clf = LinearRegression() clf.fit(X_train, Y_train) #Predict the closing price of X_prediction prediction = (clf.predict(X_prediction)) #Accuracy score of our model result = clf.score(X_test, Y_test) . Last, we can write a function to send result to our clients. The smtplib module allowed you to send emails to any internet machine with an SMTP. Check more details for SMTP. . def sendMessage(text): # If you&#39;re using Gmail to send the message, you might need to # go into the security settings of your email account and # enable the &quot;Allow less secure apps&quot; option username = &quot;Your Email Address&quot; password = &quot;The password of your email&quot; to = &quot;the email you will sent to ...@gmail.com&quot; message = text Subject = &quot;Stock Prediction&quot; msg = &#39;Subject: {} n n{}&#39;.format(Subject, text) server = smtplib.SMTP(&#39;smtp.gmail.com&#39;, 587) server.starttls() server.login(username, password) server.sendmail(username, to, msg) server.quit() print(&#39;Sent&#39;) output = (&quot; n nStock: &quot; + str(stock) + &quot; nClose Price on &quot; + str(data.loc[data.index[[-2]], &#39;Unnamed: 0&#39;].item()) + &quot;: $&quot; + str(data.loc[data.index[[-2]].item(), &#39;close&#39;]) + &quot; nPrediction for the next day closing: $%.2f&quot; % (prediction[0]) + &quot; nActuall closing: $%.2f&quot; % (Y_ans[0]) + &quot; nModel Accuracy: %.2f%%&quot; % (result*100.0)) sendMessage(output) . The email will be in this format: . . Final Application . Finally, we can add everything up to a complete application. The following code will do the job: . #collapse-show # Final application def getStocks(): #Navigating to the Yahoo stock screener driver = webdriver.Chrome( &#39;Type the directory of your chromedriver here&#39;) url = &quot;https://finance.yahoo.com/most-active&quot; driver.get(url) #Creating a stock list and iterating through the ticker names on the stock screener list #Xpath: //*[@id=&quot;scr-res-table&quot;]/div[1]/table/tbody/tr[1]/td[1]/a ticker = driver.find_element_by_xpath( &#39;//*[@id=&quot;scr-res-table&quot;]/div[1]/table/tbody/tr[1]/td[1]/a&#39;) #stock = ticker.text stock = copy.deepcopy(ticker.text) driver.quit() return predictData(stock) def sendMessage(text): # If you&#39;re using Gmail to send the message, you might need to # go into the security settings of your email account and # enable the &quot;Allow less secure apps&quot; option username = &quot;Your Email Address&quot; password = &quot;The password of your email&quot; to = &quot;the email you will sent to ...@gmail.com&quot; message = text Subject = &quot;Stock Prediction&quot; msg = &#39;Subject: {} n n{}&#39;.format(Subject, text) server = smtplib.SMTP(&#39;smtp.gmail.com&#39;, 587) server.starttls() server.login(username, password) server.sendmail(username, to, msg) server.quit() print(&#39;Sent&#39;) def predictData(stock): start = datetime(2018, 10, 2) end = datetime(2019, 10, 2) df = get_historical_data(stock, start, end, output_format=&#39;pandas&#39;, token=&quot;pk_422a359c341b427ea05864740c233fe3&quot;) csv_name = ( stock + &#39;.csv&#39;) df.to_csv(csv_name) data = pd.read_csv(csv_name) df = data[[&#39;close&#39;, &#39;high&#39;, &#39;low&#39;, &#39;open&#39;,&#39;volume&#39;, &#39;change&#39;]] df[&#39;prediction&#39;] = df[&#39;close&#39;].shift(-1) df.dropna(inplace=True) #Predicting the Stock price in the future X = np.array(df.drop([&#39;prediction&#39;], 1)) Y = np.array(df[&#39;prediction&#39;]) X = preprocessing.scale(X) X_prediction = X[-1:] Y_ans = Y[-1:] X = X[:-1] Y = Y[:-1] X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2) #Performing the Regression on the training data clf = LinearRegression() clf.fit(X_train, Y_train) prediction = (clf.predict(X_prediction)) result = clf.score(X_train, Y_train) output = (&quot; n nStock: &quot; + str(stock) + &quot; nClose Price on &quot; + str(data.loc[data.index[[-2]], &#39;Unnamed: 0&#39;].item()) + &quot;: $&quot; + str(data.loc[data.index[[-2]].item(), &#39;close&#39;]) + &quot; nPrediction for tomorrow closing: $%.2f&quot; % (prediction[0]) + &quot; nActuall closing: $%.2f&quot; % (Y_ans[0]) + &quot; nModel Accuracy: %.2f%%&quot; % (result*100.0)) sendMessage(output) if __name__ == &#39;__main__&#39;: getStocks() . . Conclusion . Thank you all for reading it, I hope this tutorial will benifit you. If you have any questions feel free to comment below. Good luck everyone! .",
            "url": "https://shudi-zhao.github.io/My_Data_Science_Blog/machine%20learning/python/selenium/stock/2021/03/17/Stock-Prediction.html",
            "relUrl": "/machine%20learning/python/selenium/stock/2021/03/17/Stock-Prediction.html",
            "date": " • Mar 17, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "Sentiment Analysis on Amazon Reviews with Python",
            "content": "Introduction . . In this blog, I will guide you through how to perform a sentiment analysis on a text data. . We will be using the Razer_mouse_reviews.csv which I scraped from Amazon using Scrapy in Python to perform our analysis. This is only a portion of the reviews. The data contains the customer reviews for the product &quot;Razer DeathAdder Essential Gaming Mouse&quot; on Amazon and their star ratings. If you want to follow me along, you can download my dataset from here Razer_mouse_reviews.csv. You can also scrape your own dataset of any product you want on Amazon follow this tutorial: scraping amazon reviews use python scrapy. In addition, this study project is inspired by this wonderfull tutorial: A Beginner’s Guide to Sentiment Analysis with Python . Data cleaning . Before we start our analysis, we need to clean up our data. . Prerequisites . First, we need to import the necessary packages. Make sure you have installed all the packages below: . numpy | pandas | matplotlib | seaborn | wordcloud | sklearn | . import numpy as np import pandas as pd import seaborn as sns color = sns.color_palette() import matplotlib.pyplot as plt %matplotlib inline from wordcloud import WordCloud, STOPWORDS from sklearn.feature_extraction.text import CountVectorizer from sklearn.linear_model import LogisticRegression from sklearn.model_selection import train_test_split from sklearn.metrics import confusion_matrix,classification_report #Optional, just want to ignore the warning text in the output import warnings from pandas.core.common import SettingWithCopyWarning warnings.simplefilter(action=&quot;ignore&quot;, category=SettingWithCopyWarning) . Read the Dataframe . df1 = pd.read_csv(&quot;Razer_mouse_reviews.csv&quot;) df1.head() . From the table below, we can see the stars are strings instead of integer rating and there are so many white line in front of each comment. . stars comment . 0 | 4.0 out of 5 stars | n n n n n n n n n n n n n For an &quot;el... | . 1 | 2.0 out of 5 stars | n n n n n n n n n n n n n Every time... | . 2 | 2.0 out of 5 stars | n n n n n n n n n n n n n The unit i... | . 3 | 1.0 out of 5 stars | n n n n n n n n n n n n n O.K., Im g... | . 4 | 1.0 out of 5 stars | n n n n n n n n n n n n n The mouse ... | . Clean the Dataframe . Next, we want to replace &quot;1.0 out of 5 stars&quot; to 1, &quot;2.0 out of 5 stars&quot; to 2, &quot;3.0 out of 5 stars&quot; to 3, &quot;4.0 out of 5 stars&quot; to 4, and &quot;5.0 out of 5 stars&quot; to 5 in the stars column. Then use str.strip() to remove all the white lines at the begainning of each comment. The code is like following: . df1 =df1.replace({&quot;1.0 out of 5 stars&quot;: 1, &#39;2.0 out of 5 stars&#39;: 2, &#39;3.0 out of 5 stars&#39;: 3, &#39;4.0 out of 5 stars&#39;: 4, &#39;5.0 out of 5 stars&#39;: 5}) #Remove the new lines at the begainning of the comment df1[&#39;comment&#39;] =df1[&#39;comment&#39;].str.strip() . Good job! Your data should be look like this: . stars comment . 0 | 4 | For an &quot;elite&quot; gaming mouse with impressive fe... | . 1 | 2 | Every time my computer starts or restarts Syna... | . 2 | 2 | The unit is just built cheap. Not the quality ... | . 3 | 1 | O.K., Im going to &quot;throw&quot; this to the air. Nob... | . 4 | 1 | The mouse left click started to break within t... | . Classify Positive and Negative Comments . We will assign comments with rating four and five as 1 which means positive sentiment, rating one and two as -1 which means negative sentiment. Since, we are not interested in rating three which is the neutral sentiment in this analysis, we will drop the comments with rating three. The code below will do the job: . df = df1[df1[&#39;stars&#39;] != 3] #add one column called sentiment contains values 1 and -1 df[&#39;sentiment&#39;] = df[&#39;stars&#39;].apply(lambda rating : +1 if rating &gt; 3 else -1) #add another column sentimentt contains values negative and positive df[&#39;sentimentt&#39;] = df[&#39;sentiment&#39;].replace({-1 : &#39;negative&#39;}) df[&#39;sentimentt&#39;] = df[&#39;sentimentt&#39;].replace({1 : &#39;positive&#39;}) . Your new dataframe will look like this: . stars comment sentiment sentimentt . 0 | 4 | For an &quot;elite&quot; gaming mouse with impressive fe... | 1 | positive | . 1 | 2 | Every time my computer starts or restarts Syna... | -1 | negative | . 2 | 2 | The unit is just built cheap. Not the quality ... | -1 | negative | . 3 | 1 | O.K., Im going to &quot;throw&quot; this to the air. Nob... | -1 | negative | . 4 | 1 | The mouse left click started to break within t... | -1 | negative | . Exploratory Data Analysis . We want explore our dataset to see if there is any interesting dicoveries and findings. Althought, our dataset only has two columns, we can still do a lot of fancy graphs and vidualizations. . Stars Counts . Next, we can viduallize the number of comments in each star rating using seaborn. Make sure you use df1 to do this plot, because we deleted rating 3 in df. . #set plot theme sns.set_theme(style=&quot;darkgrid&quot;) #Specifiy the figure size plt.figure(figsize=(15,8)) ax = sns.countplot(x=&quot;stars&quot;, data = df1, palette=&quot;Blues&quot;) ax.set_title(&quot;Number of Comments in Each Rating &quot;, fontsize=20) ax.set_xlabel(&quot;Star Rating&quot;,fontsize=15) ax.set_ylabel(&quot;Number of Comments&quot;,fontsize=15) plt.show() . The result plot looks like this: . From the plot above, we can see that the majority of the customers rating is positive. On the other hand, there are almost 100 comments are rated one star and two star. Therefore, we can build a model to predict the customers rating based on their comments. We will talk about modeling in the later section. . Sentiment Counts . Now, we can take a closer look. We want to see the number of positive comments and the number of negative comments: . sns.set_theme(style=&quot;darkgrid&quot;) plt.figure(figsize=(15,8)) ax = sns.countplot(x=&quot;sentimentt&quot;, data = df, palette=&quot;coolwarm&quot;) ax.set_title(&quot;Product Sentiment&quot;, fontsize=20) ax.set_xlabel(&quot;Sentiment&quot;,fontsize=15) ax.set_ylabel(&quot;Count&quot;,fontsize=15) plt.show() . Most Frequent Words . Next, we can use the WordCloud to find the most frequent words that appeared in the comments. . # Create stopword list stopwords = set(STOPWORDS) stopwords.update([&quot;mouse&quot;, &quot;Razer&quot;, &quot;gaming&quot;]) #Joint each word in each comment together separate by space textt = &quot; &quot;.join(review for review in df.comment) wordcloud = WordCloud(stopwords=stopwords, background_color = &quot;white&quot;).generate(textt) # Plot the worldclou to show the most frequent words in the image plt.imshow(wordcloud, interpolation=&#39;bilinear&#39;) plt.axis(&quot;off&quot;) plt.show() . The result would be look like this: . From the above image, we can see the word, &quot;great&quot;, &quot;feel&quot;, &quot;use&quot;, &quot;button&quot;, &quot;one&quot;, &quot;software&quot; are the most frequent words. Next, we can also find the most frequent words in the positive and negative comments by split the comments into positive and negative comments. . Most Frequent Words in the Positive Comments . positive = df[df[&#39;sentiment&#39;] == 1] stopwords = set(STOPWORDS) stopwords.update([&quot;mouse&quot;, &quot;Razer&quot;, &quot;gaming&quot;, &quot;use&quot;, &quot;button&quot;]) ## good and great removed because they were included in negative sentiment pos = &quot; &quot;.join(review for review in positive.comment) wordcloud2 = WordCloud(stopwords=stopwords, background_color = &quot;white&quot;).generate(pos) plt.imshow(wordcloud2, interpolation=&#39;bilinear&#39;) plt.axis(&quot;off&quot;) plt.show() . Most Frequent Words in the Nagetive Comments . negative = df[df[&#39;sentiment&#39;] == -1] negative = negative.dropna() neg = &quot; &quot;.join(review for review in negative.comment) wordcloud3 = WordCloud(stopwords=stopwords, background_color = &quot;white&quot;).generate(neg) plt.imshow(wordcloud3, interpolation=&#39;bilinear&#39;) plt.axis(&quot;off&quot;) plt.show() . Data Modeling . Finally, it comes to the most exciting part, data modeling. We want to use logistic regression to predict whether the comments is positive or negative. Before doing so, we have to do some preprocessing. . Preprocessing . Step one: Remove punctuations . You can use the following function to remove puctuations in the comments: . def remove_punctuation(text): final = &quot;&quot;.join(u for u in text if u not in (&quot;?&quot;, &quot;.&quot;, &quot;;&quot;, &quot;:&quot;, &quot;!&quot;,&#39;&quot;&#39;)) return final df[&#39;comment&#39;] = df[&#39;comment&#39;].apply(remove_punctuation) . Your new dataframe will like following table: . stars comment sentiment sentimentt . 0 | 4 | For an elite gaming mouse with impressive feat... | 1 | positive | . 1 | 2 | Every time my computer starts or restarts Syna... | -1 | negative | . 2 | 2 | The unit is just built cheap Not the quality p... | -1 | negative | . 3 | 1 | OK, Im going to throw this to the air Nobody a... | -1 | negative | . 4 | 1 | The mouse left click started to break within t... | -1 | negative | . Step two: Select the Feature for Modeling . In this example, our model only take two teatures. We will use comment to predict sentiment, 1 for positive, -1 for negative. . dfNew = df[[&#39;comment&#39;,&#39;sentiment&#39;]] dfNew.head() . Your selected features are following columns: . comment sentiment . 0 | For an elite gaming mouse with impressive feat... | 1 | . 1 | Every time my computer starts or restarts Syna... | -1 | . 2 | The unit is just built cheap Not the quality p... | -1 | . 3 | OK, Im going to throw this to the air Nobody a... | -1 | . 4 | The mouse left click started to break within t... | -1 | . Step three: Split Train and Test Data . We can randomly split our data using the train_test_split function in the sklearn package, which already imported in the very begining. 80% of our data will be used for training, and 20% will be used for testing. Thre are many other methods to split your data, feel free to use your own way. . train ,test = train_test_split(df,test_size=0.2) . Step four: Vectorize Comments . In this step, we want to Convert the collection of text comments to a matrix of token counts, because the logistic regression algorithm cannot understand text. . We will use a count vectorizer from the Scikit-learn library to transform the text comments into a bag of words model, which will find the unique words in each comment, and count the occurence for each word in each comment. . vectorizer = CountVectorizer(token_pattern=r&#39; b w+ b&#39;) #vectorize both train data and test data train_matrix = vectorizer.fit_transform(train[&#39;comment&#39;]) test_matrix = vectorizer.transform(test[&#39;comment&#39;]) . Logistic Regression . Finally, we can use the Logistic Regression from the Scikit-learn library to fit our trainning data and make predictions using our test data. . lr = LogisticRegression() #Split target and independent variables X_train = train_matrix X_test = test_matrix y_train = train[&#39;sentiment&#39;] y_test = test[&#39;sentiment&#39;] #Fit model on data lr.fit(X_train,y_train) #Make predictions predictions = lr.predict(X_test) . Data Validation . We can test our model accuracy use the confusion matrix, which imported from the sklearn package in the very beginning: . new = np.asarray(y_test) cf_matrix = confusion_matrix(predictions,y_test) #Display our confusion matrix in a heatmap: group_names = [&quot;True Negative&quot;,&quot;False Positive&quot;,&quot;False Negative&quot;,&quot;True Positive&quot;] group_counts = [&quot;{0:0.0f}&quot;.format(value) for value in cf_matrix.flatten()] group_percentages = [&quot;{0:.2%}&quot;.format(value) for value in cf_matrix.flatten()/np.sum(cf_matrix)] labels = [f&quot;{v1} n{v2} n{v3}&quot; for v1, v2, v3 in zip(group_names,group_counts,group_percentages)] labels = np.asarray(labels).reshape(2,2) sns.heatmap(cf_matrix, annot=labels, fmt=&#39;&#39;, cmap=&#39;Blues&#39;) plt.show() . We can also generate a classification report to validate our model accuracy: . print(classification_report(predictions,y_test)) . precision recall f1-score support -1 0.67 0.86 0.75 14 1 0.97 0.91 0.94 70 accuracy 0.90 84 macro avg 0.82 0.89 0.85 84 weighted avg 0.92 0.90 0.91 84 . The overall accuracy of the model on the test data is around 90%, which is pretty good since our dataset is not very large. . Thank you for reading it, I hope this tutorial will help you to understand the basics of the sentiment analysis, WorldCloud, and Logistic Regression. If you have any questions feel free to comment below. Good luck everyone, you are on the right track. .",
            "url": "https://shudi-zhao.github.io/My_Data_Science_Blog/sentiment%20analysis/wordcloud/python/machine%20learning/data%20mining/2021/03/14/Sentiment-Analysis.html",
            "relUrl": "/sentiment%20analysis/wordcloud/python/machine%20learning/data%20mining/2021/03/14/Sentiment-Analysis.html",
            "date": " • Mar 14, 2021"
        }
        
    
  
    
        ,"post3": {
            "title": "My Resume",
            "content": "EDUCATION . Fei Tian College—Middletown, NY Graduating: May 2022 . Bachelor of Science in Data Science Cumulative GPA: 3.93/4.00 . Two-time winner of the Provost’s Outstanding Student Award . | Three-time winner of the Academic Excellence Award . | . Thomas Kelly High School, Chicago, IL June 2018 . Received: The Seal of Biliteracy—English and Mandarin Unweighted GPA: 3.90/4.00 . COURSES . Data Mining, Statistical Computing and Graphics, Database, Cloud Computing and Big Data, Data Structure Algorithms, Linear Regression, Statistical Theory and Method, Business Data Analytics, Microeconomics . PROJECTS . Data Science Project with AWS EC2 and RDS Feb 2020–Aug 2020 . Created a comprehensive system flight booking system with a MySQL database backend to store user login, flight information, and booking data . | Used Python flask and HTML, ran on Amazon EC2, created a user-friendly front-end UI . | . Coronavirus Tracker (Tableau) May 2020–Aug 2020 . Developed an interactive animated Tableau time series analysis dashboard with waterfall charts, line charts, and maps to visualize COVID-19 spreads within the U.S. and globally . | Data manipulation in Excel using advanced formulas (VLOOKUP, Index match, pivot table) . | . Data Analysis in R Project Lead Sep 2019–Dec 2019 . Spearheaded linear regression to forecast SAT score and correlation analysis using the time spent studying and GPA . | Performed data manipulation, data cleaning, normalization, and prescriptive analytics . | . EXPERIENCE . Arc’teryx, Central Valley, NY May 2019–Present . Product Guide . Results-oriented and increase 30% sales revenue and 35% customer service by leveraging my communication, bilingual literacy and by analyzing foot traffic at the mall  | . Managing large amounts of incoming calls and follow company escalation policy . | Identifying causes of customer problems and complaints with the best solution . | . SKILLS . Advanced programming skills in MySQL, Python, Java, and HTML . | Data analysis and visualization in R ggplot, Tableau, Python Matplotlib and Pandas . | Advanced Proficiency in Microsoft Office, Excel, PowerPoint and Word. . | Strong negotiation, financial analytical skill, qualitative and quantitative research skills . | Project management, public relations, teamwork, detail oriented, professionalism, social skill . | .",
            "url": "https://shudi-zhao.github.io/My_Data_Science_Blog/2021/03/10/My-Resume.html",
            "relUrl": "/2021/03/10/My-Resume.html",
            "date": " • Mar 10, 2021"
        }
        
    
  
    
        ,"post4": {
            "title": "Women Shoe Prices",
            "content": "Women&#39;s Shoe Prices . Introduction . This is a list of 10,000 women&#39;s shoes and their product information provided by Datafiniti&#39;s Product Database. . The dataset includes shoe name, brand, price, and more. Each shoe will have an entry for each price found for it and some shoes may have multiple entries. . Import libraries . import numpy as np import pandas as pd import matplotlib.pyplot as plt %matplotlib inline . Loading Data . WomenShoe = pd.read_csv(&#39;Women_Shoe_Price.csv&#39;, parse_dates = [&#39;dateAdded&#39;, &#39;dateUpdated&#39;, &#39;prices.dateAdded&#39;, &#39;prices.dateSeen&#39;] ) . WomenShoe.shape . (19034, 47) . Data Cleaning . Missing Values . miss_val = (WomenShoe.isnull().sum()/len(WomenShoe)*100).sort_values(ascending=False) miss_val[miss_val&gt;0] . count 100.000000 flavors 100.000000 isbn 100.000000 websiteIDs 100.000000 quantities 100.000000 prices.count 100.000000 prices.source 100.000000 prices.flavor 100.000000 prices.warranty 99.926447 prices.availability 99.338027 prices.size 97.336345 prices.color 97.157718 prices.returnPolicy 96.075444 weight 95.649890 reviews 94.672691 asins 88.410213 dimension 87.296417 prices.shipping 76.037617 prices.offer 64.038037 skus 62.782389 sizes 56.567196 manufacturer 54.555007 ean 48.450142 upc 43.963434 descriptions 43.411789 colors 37.574866 prices.condition 35.767574 merchants 28.133866 features 25.927288 prices.merchant 24.939582 manufacturerNumber 16.502049 imageURLs 6.320269 brand 3.325628 dtype: float64 . (miss_val[miss_val==0].index) . Index([&#39;prices.isSale&#39;, &#39;categories&#39;, &#39;dateAdded&#39;, &#39;dateUpdated&#39;, &#39;sourceURLs&#39;, &#39;prices.dateSeen&#39;, &#39;prices.sourceURLs&#39;, &#39;prices.dateAdded&#39;, &#39;keys&#39;, &#39;name&#39;, &#39;prices.amountMin&#39;, &#39;prices.amountMax&#39;, &#39;prices.currency&#39;, &#39;id&#39;], dtype=&#39;object&#39;) . WomenShoe = WomenShoe.drop(columns= [&#39;id&#39;, &#39;asins&#39;, &#39;count&#39;, &#39;descriptions&#39;, &#39;dimension&#39;,&#39;ean&#39;, &#39;flavors&#39;, &#39;isbn&#39;,&#39;manufacturer&#39;, &#39;manufacturerNumber&#39;, &#39;merchants&#39;, &#39;keys&#39;, &#39;imageURLs&#39;, &#39;prices.availability&#39;, &#39;prices.color&#39;, &#39;prices.condition&#39;, &#39;prices.count&#39;, &#39;prices.flavor&#39;, &#39;prices.merchant&#39;, &#39;prices.offer&#39;, &#39;prices.returnPolicy&#39;, &#39;prices.shipping&#39;, &#39;prices.size&#39;, &#39;prices.source&#39;, &#39;prices.warranty&#39;, &#39;quantities&#39;, &#39;reviews&#39;, &#39;skus&#39;, &#39;upc&#39;, &#39;websiteIDs&#39;, &#39;weight&#39;, &#39;prices.sourceURLs&#39;,&#39;prices.dateAdded&#39;, &#39;prices.dateSeen&#39;, &#39;dateUpdated&#39;, &#39;sizes&#39;, &#39;sourceURLs&#39;] ) #Check the new dimension of the dataset WomenShoe.shape . (19034, 10) . WomenShoe = WomenShoe.rename(columns={&quot;prices.amountMin&quot;: &quot;MinPrice&quot;, &quot;prices.amountMax&quot;: &quot;MaxPrice&quot;, &quot;prices.currency&quot;: &quot;currency&quot;}) WomenShoe.head(8) . brand categories colors dateAdded features name MinPrice MaxPrice currency prices.isSale . 0 | Zoot | Shoes,Clothing,Women&#39;s Shoes,All Women&#39;s Shoes | Blue,Multicolor | 2016-11-11 09:49:00+00:00 | [{&quot;key&quot;:&quot;Season&quot;,&quot;value&quot;:[&quot;All-Season&quot;]},{&quot;key... | Zoot Tt Trainer 2.0 Round Toe Synthetic Sne... | 71.99 | 71.99 | USD | True | . 1 | Zoot | Shoes,Clothing,Women&#39;s Shoes,All Women&#39;s Shoes | Blue,Multicolor | 2016-11-11 09:49:00+00:00 | [{&quot;key&quot;:&quot;Season&quot;,&quot;value&quot;:[&quot;All-Season&quot;]},{&quot;key... | Zoot Tt Trainer 2.0 Round Toe Synthetic Sne... | 250.00 | 250.00 | USD | False | . 2 | Zoot | Shoes,Clothing,Women&#39;s Shoes,All Women&#39;s Shoes | Blue,Multicolor | 2016-11-11 09:49:00+00:00 | [{&quot;key&quot;:&quot;Season&quot;,&quot;value&quot;:[&quot;All-Season&quot;]},{&quot;key... | Zoot Tt Trainer 2.0 Round Toe Synthetic Sne... | 75.99 | 75.99 | USD | True | . 3 | Zoot | Shoes,Clothing,Women&#39;s Shoes,All Women&#39;s Shoes | Blue,Multicolor | 2016-11-11 09:49:00+00:00 | [{&quot;key&quot;:&quot;Season&quot;,&quot;value&quot;:[&quot;All-Season&quot;]},{&quot;key... | Zoot Tt Trainer 2.0 Round Toe Synthetic Sne... | 79.98 | 79.98 | USD | True | . 4 | Wild Pair | Shoes,Women&#39;s Shoes,Clothing,All Women&#39;s Shoes | Brown | 2016-11-16 12:56:36+00:00 | [{&quot;key&quot;:&quot;Heel Height&quot;,&quot;value&quot;:[&quot;High (3 in. an... | Wild Pair Colfax Women Peep-toe Synthetic Bro... | 26.98 | 26.98 | USD | True | . 5 | Wild Pair | Shoes,Women&#39;s Shoes,Clothing,All Women&#39;s Shoes | Brown | 2016-11-16 12:56:36+00:00 | [{&quot;key&quot;:&quot;Heel Height&quot;,&quot;value&quot;:[&quot;High (3 in. an... | Wild Pair Colfax Women Peep-toe Synthetic Bro... | 15.99 | 15.99 | USD | True | . 6 | Wild Pair | Shoes,Women&#39;s Shoes,Clothing,All Women&#39;s Shoes | Brown | 2016-11-16 12:56:36+00:00 | [{&quot;key&quot;:&quot;Heel Height&quot;,&quot;value&quot;:[&quot;High (3 in. an... | Wild Pair Colfax Women Peep-toe Synthetic Bro... | 90.00 | 90.00 | USD | False | . 7 | Sutton Studio | Women&#39;s Suits &amp; Sets,Women&#39;s Clothing,All Wome... | Pink | 2016-01-02 03:16:24+00:00 | [{&quot;key&quot;:&quot;Style&quot;,&quot;value&quot;:[&quot;Suit jackets&quot;]},{&quot;ke... | Sutton Studio Women&#39;s 100 Cashmere Blazer Jacket | 318.00 | 318.00 | USD | False | . Correct Data Types . Actually, the data types for dateAdded, MinPrice, MaxPrice, and prices.isSale are all object when I first process this dataset. I tried to convert MinPrice and MaxPrice to float, but it didn&#39;t work. I looked at the data in Excel, I found there are so many rows with data that are overlapped with other columns, so I had to go over those 19,034 rows to find and correct the data manually. . WomenShoe.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 19034 entries, 0 to 19033 Data columns (total 10 columns): brand 18401 non-null object categories 19034 non-null object colors 11882 non-null object dateAdded 19034 non-null datetime64[ns, UTC] features 14099 non-null object name 19034 non-null object MinPrice 19034 non-null float64 MaxPrice 19034 non-null float64 currency 19034 non-null object prices.isSale 19034 non-null bool dtypes: bool(1), datetime64[ns, UTC](1), float64(2), object(6) memory usage: 1.3+ MB . Currency Change . WomenShoe[&#39;average_price&#39;] = (WomenShoe[&#39;MinPrice&#39;] + WomenShoe[&#39;MaxPrice&#39;])/2 #drop MinPrice and MaxPrice columns WomenShoe.drop(columns=[&#39;MinPrice&#39;, &#39;MaxPrice&#39;], axis=1, inplace=True) WomenShoe.head() #9 columns left . brand categories colors dateAdded features name currency prices.isSale average_price . 0 | Zoot | Shoes,Clothing,Women&#39;s Shoes,All Women&#39;s Shoes | Blue,Multicolor | 2016-11-11 09:49:00+00:00 | [{&quot;key&quot;:&quot;Season&quot;,&quot;value&quot;:[&quot;All-Season&quot;]},{&quot;key... | Zoot Tt Trainer 2.0 Round Toe Synthetic Sne... | USD | True | 71.99 | . 1 | Zoot | Shoes,Clothing,Women&#39;s Shoes,All Women&#39;s Shoes | Blue,Multicolor | 2016-11-11 09:49:00+00:00 | [{&quot;key&quot;:&quot;Season&quot;,&quot;value&quot;:[&quot;All-Season&quot;]},{&quot;key... | Zoot Tt Trainer 2.0 Round Toe Synthetic Sne... | USD | False | 250.00 | . 2 | Zoot | Shoes,Clothing,Women&#39;s Shoes,All Women&#39;s Shoes | Blue,Multicolor | 2016-11-11 09:49:00+00:00 | [{&quot;key&quot;:&quot;Season&quot;,&quot;value&quot;:[&quot;All-Season&quot;]},{&quot;key... | Zoot Tt Trainer 2.0 Round Toe Synthetic Sne... | USD | True | 75.99 | . 3 | Zoot | Shoes,Clothing,Women&#39;s Shoes,All Women&#39;s Shoes | Blue,Multicolor | 2016-11-11 09:49:00+00:00 | [{&quot;key&quot;:&quot;Season&quot;,&quot;value&quot;:[&quot;All-Season&quot;]},{&quot;key... | Zoot Tt Trainer 2.0 Round Toe Synthetic Sne... | USD | True | 79.98 | . 4 | Wild Pair | Shoes,Women&#39;s Shoes,Clothing,All Women&#39;s Shoes | Brown | 2016-11-16 12:56:36+00:00 | [{&quot;key&quot;:&quot;Heel Height&quot;,&quot;value&quot;:[&quot;High (3 in. an... | Wild Pair Colfax Women Peep-toe Synthetic Bro... | USD | True | 26.98 | . WomenShoe.currency.value_counts() . USD 18199 CAD 638 AUD 118 EUR 64 GBP 15 Name: currency, dtype: int64 . def curr_change(x): if x.currency == &#39;AUD&#39;: x.average_price = x.average_price * 0.73 if x.currency == &#39;CAD&#39;: x.average_price = x.average_price * 0.76 if x.currency == &#39;EUR&#39;: x.average_price = x.average_price * 1.19 if x.currency == &#39;GBP&#39;: x.average_price = x.average_price * 1.33 return x . WomenShoe[WomenShoe[&#39;currency&#39;] != &quot;USD&quot;].head() . brand categories colors dateAdded features name currency prices.isSale average_price . 42 | NaN | Shoes &amp; Handbags,Shoes,Women,Athletic | NaN | 2016-08-27 18:41:34+00:00 | NaN | Sneed- Customizable Women&#39;s Dance Shoes Latin/... | CAD | False | 88.00 | . 80 | DREAM GIRL | Clothing, Shoes &amp; Accessories,Costumes, Reenac... | Greek Roman | 2016-03-31 20:29:11+00:00 | [{&quot;key&quot;:&quot;Manufacturer Part Number&quot;,&quot;value&quot;:[&quot;5... | Adult Womens Sexy Greek Roman Goddess Toga Fan... | AUD | False | 39.99 | . 117 | NaN | Athletic,Women,Shoes,Shoes &amp; Handbags | White,Blue,Black,Burgundy | 2015-11-16 02:56:40+00:00 | NaN | Yu&amp;yu Women&#39;s Shoes Pointed Toe Chunky Heel Ox... | CAD | False | 154.22 | . 128 | NaN | Shoes &amp; Handbags,Shoes,Women,Athletic | NaN | 2016-08-27 18:41:27+00:00 | NaN | Sneed- Customizable Women&#39;s Dance Shoes Latin/... | CAD | False | 88.00 | . 142 | NaN | Athletic,Women,Shoes,Shoes &amp; Handbags | Blue,Red,Black | 2015-11-16 02:55:17+00:00 | NaN | Yu&amp;yu Women&#39;s Shoes Round Toe Flat Heel Flats ... | CAD | False | 57.32 | . WomenShoe = WomenShoe.apply(curr_change, axis=1) currency_change = WomenShoe[WomenShoe[&#39;currency&#39;] != &quot;USD&quot;] currency_change.head() . brand categories colors dateAdded features name currency prices.isSale average_price . 42 | NaN | Shoes &amp; Handbags,Shoes,Women,Athletic | NaN | 2016-08-27 18:41:34+00:00 | NaN | Sneed- Customizable Women&#39;s Dance Shoes Latin/... | CAD | False | 66.8800 | . 80 | DREAM GIRL | Clothing, Shoes &amp; Accessories,Costumes, Reenac... | Greek Roman | 2016-03-31 20:29:11+00:00 | [{&quot;key&quot;:&quot;Manufacturer Part Number&quot;,&quot;value&quot;:[&quot;5... | Adult Womens Sexy Greek Roman Goddess Toga Fan... | AUD | False | 29.1927 | . 117 | NaN | Athletic,Women,Shoes,Shoes &amp; Handbags | White,Blue,Black,Burgundy | 2015-11-16 02:56:40+00:00 | NaN | Yu&amp;yu Women&#39;s Shoes Pointed Toe Chunky Heel Ox... | CAD | False | 117.2072 | . 128 | NaN | Shoes &amp; Handbags,Shoes,Women,Athletic | NaN | 2016-08-27 18:41:27+00:00 | NaN | Sneed- Customizable Women&#39;s Dance Shoes Latin/... | CAD | False | 66.8800 | . 142 | NaN | Athletic,Women,Shoes,Shoes &amp; Handbags | Blue,Red,Black | 2015-11-16 02:55:17+00:00 | NaN | Yu&amp;yu Women&#39;s Shoes Round Toe Flat Heel Flats ... | CAD | False | 43.5632 | . Uniform Brand Names . WomenShoe.brand.value_counts() . Ralph Lauren 543 Nike 367 TOMS 327 MUK LUKS 237 Easy Spirit 232 ... Zerouv 1 301-42 SW-SM 1 Lee Rider 1 Marchon 1 WIG 1 Name: brand, Length: 2141, dtype: int64 . brand_map = {&#39;a2 by aerosoles&#39;:&#39;aerosoles&#39;, &quot;what&#39;s what by aerosoles&quot;:&#39;aerosoles&#39;, &#39;adidas outdoor&#39;:&#39;adidas&#39;, &#39;adriana new york&#39;:&#39;adriana&#39;, &#39;alexander mcqueen by puma&#39;:&#39;puma&#39;, &#39;alexander mcqueen&#39;:&#39;puma&#39;, &#39;alpine&#39;:&#39;alpine swiss&#39;, &#39;anne klein ak&#39;:&#39;anne klein&#39;, &#39;anne klein sport&#39;:&#39;anne klein&#39;, &#39;annie shoes&#39;:&#39;annie&#39;, &#39;athena&#39;:&#39;athena alexander&#39;, &#39;babe.&#39;:&#39;babe&#39;, &#39;baretraps&#39;:&#39;bare traps&#39;, &#39;bcbg max azria&#39;:&#39;bcbg&#39;, &#39;bcbg paris&#39;:&#39;bcbg&#39;, &#39;bcbgeneration&#39;:&#39;bcbg&#39;, &#39;beacon shoes&#39;:&#39;beacon&#39;, &#39;bebe sport&#39;:&#39;bebe&#39;, &#39;bebe stu&#39;:&#39;bebe&#39;, &#39;belle by sigerson morrison&#39;:&#39;belle&#39;, &#39;belle sigerson morrison&#39;:&#39;belle&#39;, &#39;bernie mev&#39;:&#39;bernie&#39;, &#39;bernie mev.&#39;:&#39;bernie&#39;, &#39;bettye by bettye muller&#39;:&#39;bettye muller&#39;, &#39;bettye by bettye muller &#39;:&#39;bettye muller&#39;, &quot;breckelle&#39;s&quot;:&#39;breckelles&#39;, &#39;callaway footwear&#39;:&#39;callaway&#39;, &#39;calvin klein ck&#39;:&#39;calvin klein&#39;, &#39;calvin klein jeans&#39;:&#39;calvin klein&#39;, &#39;carlos by carlos santana&#39;:&#39;carlos santana&#39;, &#39;charles by charles david&#39;:&#39;charels david&#39;, &#39;see by chloe&#39;:&#39;chloe&#39;, &#39;clarks artisan&#39;:&#39;clarks&#39;, &#39;clarks artisan collection&#39;:&#39;clarks&#39;, &#39;clarks collection&#39;:&#39;clarks&#39;, &#39;cobb hill by new balance&#39;:&#39;new balance&#39;, &#39;cobb hill&#39;:&#39;new balance&#39;, &#39;maria sharapova by cole haan&#39;: &#39;cole hann&#39;, &quot;corky&#39;s&quot;:&#39;corkys&#39;, &quot;corky&#39;s footwear&quot;:&#39;corkys&#39;, &quot;corkys footwear, inc.&quot;:&#39;corkys&#39;, &#39;dearforms&#39;:&#39;dearfoams&#39;, &#39;df by dearfoams&#39;:&#39;dearfoams&#39;, &#39;derek lam 10 crosby&#39;:&#39;derek lam&#39;, &#39;diba.true&#39;:&#39;dibatrue&#39;, &#39;dolce&amp;gabbana&#39;:&#39;dolce and gabbana&#39;, &#39;dolce &amp; gabbana&#39;:&#39;dolce and gabbana&#39;, &#39;dolce by mojo moxy&#39;:&#39;dolce and gabbana&#39;, &#39;dolce vita&#39;:&#39;dolce and gabbana&#39;, &#39;dv8 by dolce vita&#39;:&#39;dolce and gabbana&#39;, &#39;dv by dolce vita&#39;:&#39;dolce and gabbana&#39;, &quot;dr. scholl&#39;s&quot;:&#39;dr scholls&#39;, &quot;dr. martens air wair&quot;:&#39;drmartens&#39;, &#39;drew shoe&#39;:&#39;drew&#39;, &#39;easy spirit e360&#39;:&#39;easy spirit&#39;, &#39;easy spirit.&#39;:&#39;easy spirit&#39;, &#39;ellie shoes&#39;:&#39;ellie&#39;, &#39;emu australia&#39;:&#39;emu&#39;, &#39;fergie footwear&#39;:&#39;fergie&#39;, &#39;forever collectible&#39;:&#39;forever&#39;, &#39;forever link&#39;:&#39;forever&#39;, &#39;fourever funky&#39;:&#39;forever&#39;, &#39;sarto by franco sarto&#39;:&#39;franco sarto&#39;, &#39;ferriniusa&#39;:&#39;ferrini&#39;, &#39;fitflop&#39;:&#39;fit flop&#39;, &#39;funtasma by pleaser&#39;:&#39;funtasma&#39;, &#39;g by guess&#39;:&#39;guess&#39;, &#39;gc shoes&#39;:&#39;gc&#39;, &#39;genuine grip footwear&#39;:&#39;genuine grip&#39;, &quot;hogan by tod&#39;s&quot;:&#39;hogan&#39;, &#39;soft style by hush puppies&#39;:&#39;hush puppies&#39;, &#39;ilse jacobsen hornbaek&#39;:&#39;ilse jacobson&#39;, &#39;isaacmizrahi&#39;:&#39;isaac mizrahi&#39;, &#39;italian shoe makers&#39;:&#39;italian comfort&#39;, &#39;j.renee&#39;:&#39;j. renee&#39;, &#39;jbu by jambu&#39;:&#39;jambu&#39;, &#39;josefseibel&#39;:&#39;josef siebel&#39;, &#39;justin blair&#39;:&#39;justin&#39;, &#39;justin boots&#39;:&#39;justin&#39;, &#39;justin gypsy&#39;:&#39;justin&#39;, &#39;kate spade new york&#39;:&#39;kate spade&#39;, &#39;kenneth cole reaction&#39;:&#39;kenneth cole&#39;, &#39;kenneth cole ny&#39;:&#39;kenneth cole&#39;, &#39;kenneth cole new york&#39;:&#39;kenneth cole&#39;, &#39;unlisted kenneth cole&#39;:&#39;kenneth cole&#39;, &#39;lamo sheepskin inc&#39;:&#39;lamo&#39;, &#39;lifestride&#39;:&#39;lifes tride&#39;, &#39;luoluo&#39;:&#39;luo luo&#39;, &#39;marc fisher ltd&#39;:&#39;marc fisher&#39;, &#39;mia heritage&#39;:&#39;mia&#39;, &#39;micahel kors&#39;:&#39;michael kors&#39;, &#39;michael michael kors&#39;:&#39;michael kors&#39;, &#39;mobils by mephisto&#39;:&#39;mephisto&#39;, &#39;top moda&#39;:&#39;moda&#39;, &#39;moda essentials&#39;:&#39;moda&#39;, &#39;everybody by bz moda&#39;:&#39;moda&#39;, &#39;muk luks a la mode&#39;:&#39;muk luks&#39;, &#39;munro american&#39;:&#39;munro&#39;, &#39;naot footwear&#39;:&#39;naot&#39;, &#39;new@titude&#39;:&#39;new attitude&#39;, &#39;new@ttitude&#39;:&#39;new attitude&#39;, &#39;nina originals&#39;:&#39;nina&#39;, &#39;nine west vintage america collection&#39;:&#39;nine west&#39;, &#39;nufoot���&#39;:&#39;nufoot&#39;, &#39;pleaser shoes&#39;:&#39;pleaser&#39;, &#39;pleaser usa, inc.&#39;:&#39;pleaser&#39;, &#39;pleaserusa&#39;:&#39;pleaser&#39;, &#39;rachel&#39;:&#39;rachel roy&#39;, &#39;rachel rachel roy &#39;:&#39;rachel roy&#39;, &#39;lauren by ralph lauren&#39;:&#39;ralph lauren&#39;, &#39;lauren ralph lauren&#39;:&#39;ralph lauren&#39;, &#39;lauren lorraine&#39;:&#39;ralph lauren&#39;, &#39;polo ralph lauren&#39;:&#39;ralph lauren&#39;,&#39;ralph lauren denim supply&#39;:&#39;ralph lauren&#39;, &#39;rieker-antistress&#39;:&#39;rieker&#39;, &#39;rocket dog brands llc&#39;:&#39;rocket dog&#39;, &#39;sanita clogs&#39;:&#39;sanita&#39;, &#39;ferragamo&#39;:&#39;salvatore ferragamo&#39;, &#39;skechers usa&#39;:&#39;skechers&#39;, &#39;sperry top sider&#39;:&#39;sperry&#39;, &#39;sperry top-sider&#39;:&#39;sperry&#39;, &quot;l&#39;artiste by spring step&quot;:&#39;spring step&#39;, &quot;flexus by spring step&quot;:&#39;spring step&#39;, &quot;patrizia by spring step &quot;:&#39;spring step&#39;, &quot;patrizia pepe&quot;:&#39;spring step&#39;, &quot;patrizia&quot;:&#39;spring step&#39;, &#39;steven steve madden&#39;:&#39;steve madden&#39;, &#39;style &amp; co.&#39;:&#39;style and co&#39;, &#39;timberland earthkeepers&#39;:&#39;timberland&#39;, &#39;timberland pro&#39;:&#39;timberland&#39;, &#39;toms shoes&#39;:&#39;toms&#39;, &#39;tony lama boot co.&#39;:&#39;tony lama&#39;, &#39;totes isotoner&#39;:&#39;totes&#39;, &#39;trotter&#39;:&#39;trotters&#39;, &#39;ugg australia&#39;:&#39;ugg&#39;, &#39;famous name brand&#39;:&#39;unbranded&#39;, &#39;generic&#39;:&#39;unbranded&#39;, &#39;generic surplus&#39;:&#39;unbranded&#39;, &#39;non-branded&#39;:&#39;unbranded&#39;, &#39;not applicable&#39;:&#39;unbranded&#39;, &#39;not rated&#39;:&#39;unbranded&#39;, &#39;lucky brand&#39;:&#39;unbranded&#39;, &#39;lucky brand&#39;:&#39;unbranded&#39;, &#39;very fine dance shoes&#39;:&#39;unbranded&#39;, &#39;valentino noir&#39;:&#39;valentino&#39;, &#39;victoria k.&#39;:&#39;victoria&#39;, &#39;vince camuto&#39;:&#39;vince&#39;, &#39;vionic by orthaheel&#39;:&#39;vionic&#39;, &#39;vionic with orthaheel technology&#39;:&#39;vionic&#39;, &#39;elites by walking cradles&#39;:&#39;walking cradles&#39;, &#39;elites&#39;:&#39;walking cradles&#39;,&#39;mark lemp by walking cradles&#39;:&#39;walking cradles&#39;, &#39;rose petals by walking cradles&#39;:&#39;walking cradles&#39;, &#39;the walking cradle company&#39;:&#39;walking cradles&#39; } . Reference this note book https://www.kaggle.com/ashishg21/data-cleaning-and-some-analysis-shoe-prices. . WomenShoe[&#39;brand&#39;] = WomenShoe[&#39;brand&#39;].str.lower() # replace the brand names with the dictionary WomenShoe[&#39;brand_clean&#39;] = WomenShoe[&#39;brand&#39;].replace(brand_map) . WomenShoe.brand_clean.value_counts() . ralph lauren 606 nike 369 toms 328 easy spirit 270 muk luks 238 ... boss 1 lrl lauren jeans co. 1 mensusa.com 1 katuo 1 star bay 1 Name: brand_clean, Length: 1897, dtype: int64 . EDA . Color &amp; Price Analysis . What are the 10 most popular colors for women&#39;s shoe? . WomenShoe.colors.value_counts() . Black 1687 Brown 779 Beige 507 Blue 335 White 323 ... Mint,White,Nude,Neon Fuchsia 1 Powder Blue,Fig Purple,Chambray,Dusk 1 WhitePink 1 Black,Natural,Beige 1 Yellow,Beige,Orange 1 Name: colors, Length: 2473, dtype: int64 . color_rank = WomenShoe.colors.value_counts().head(10) # append 10 colors with the highest count in to a list rows = [] for i in range(10): rows.append([color_rank.index[i], color_rank[i]]) # convert the list to a pandas dataframe Col_Rank = pd.DataFrame(rows, columns=[&quot;color&quot;, &quot;count&quot;]) . plt.style.use(&#39;ggplot&#39;) fig, ax = plt.subplots(figsize=(12, 5)) Col_Rank = Col_Rank.sort_values(&#39;count&#39;) ax.barh(Col_Rank[&#39;color&#39;], Col_Rank[&#39;count&#39;], color= [&#39;pink&#39;, &#39;green&#39;, &#39;silver&#39;, &#39;red&#39;, &#39;gray&#39;, &#39;white&#39;, &#39;blue&#39;, &#39;beige&#39;, &#39;brown&#39;, &#39;black&#39;]) ax.set_ylabel(&quot;Color&quot;) ax.set_xlabel(&quot;Count&quot;) ax.set_title(&quot;Top 10 Color Count&quot;) plt.show() . Does the mediem price of each color also follow the color rank in the plot above? . Black = WomenShoe[WomenShoe[&#39;colors&#39;] == &quot;Black&quot;] Brown = WomenShoe[WomenShoe[&#39;colors&#39;] == &quot;Brown&quot;] Beige = WomenShoe[WomenShoe[&#39;colors&#39;] == &quot;Beige&quot;] Blue = WomenShoe[WomenShoe[&#39;colors&#39;] == &quot;Blue&quot;] White = WomenShoe[WomenShoe[&#39;colors&#39;] == &quot;White&quot;] Gray = WomenShoe[WomenShoe[&#39;colors&#39;] == &quot;Gray&quot;] # Make a copy of dataframes for each color Black1 = WomenShoe[WomenShoe[&#39;colors&#39;] == &quot;Black&quot;] Brown1 = WomenShoe[WomenShoe[&#39;colors&#39;] == &quot;Brown&quot;] Beige1 = WomenShoe[WomenShoe[&#39;colors&#39;] == &quot;Beige&quot;] Blue1 = WomenShoe[WomenShoe[&#39;colors&#39;] == &quot;Blue&quot;] White1 = WomenShoe[WomenShoe[&#39;colors&#39;] == &quot;White&quot;] Gray1 = WomenShoe[WomenShoe[&#39;colors&#39;] == &quot;Gray&quot;] . def outlier(x): y = (x[&#39;average_price&#39;].quantile(0.75) - x.average_price.quantile(0.25)) * 1.5 + x.average_price.quantile(0.75) return y print(outlier(Black)) print(outlier(Brown)) print(outlier(Beige)) print(outlier(Blue)) print(outlier(White)) print(outlier(Gray)) . 166.95499999999998 204.25250000000003 153.495 120.86250000000001 144.735 190.25250000000003 . Black.drop(Black[Black[&#39;average_price&#39;] &gt; 167].index, inplace = True) #Remove the outliers for Brown shoes Brown.drop(Brown[Brown[&#39;average_price&#39;] &gt; 205].index, inplace = True) #Remove the outliers for Beige shoes Beige.drop(Beige[Beige[&#39;average_price&#39;] &gt; 154].index, inplace = True) #Remove the outliers for Blue shoes Blue.drop(Blue[Blue[&#39;average_price&#39;] &gt; 121].index, inplace = True) #Remove the outliers for White shoes White.drop(White[White[&#39;average_price&#39;] &gt; 145].index, inplace = True) #Remove the outliers for Beige shoes Gray.drop(Gray[Gray[&#39;average_price&#39;] &gt; 191].index, inplace = True) . /Applications/Jupter/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py:4102: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy errors=errors, . plt.style.use(&#39;ggplot&#39;) fig, ax = plt.subplots(figsize=(15, 7)) ax.boxplot([Black[&quot;average_price&quot;], Brown[&quot;average_price&quot;], Beige[&quot;average_price&quot;], Blue[&quot;average_price&quot;], White[&quot;average_price&quot;], Gray[&quot;average_price&quot;]]) ax.set_xticklabels([&quot;Black&quot;, &quot;Brown&quot;, &quot;Beige&quot;, &quot;Blue&quot;, &quot;White&quot;, &quot;Gray&quot;]) ax.set_ylabel(&quot;Price&quot;) ax.set_xlabel(&quot;Color&quot;) ax.set_title(&#39;Box Plot for the Top 6 Colors&#39;) plt.show() . How prices are distributed in the top six colors? . plt.style.use(&#39;ggplot&#39;) fig, ax = plt.subplots(3, 2, figsize=(15, 8)) ax[0, 0].hist(Black1[&quot;average_price&quot;], bins=100, range=(0,500), color = &#39;black&#39;) ax[0,0].set_title(&#39;Black Shoes&#39;) ax[0,0].set_ylabel(&#39;Frequency&#39;) ax[0, 1].hist(Brown1[&quot;average_price&quot;], bins=100, range=(0,500), color = &#39;Brown&#39;) ax[0,1].set_title(&#39;Brown Shoes&#39;) ax[1, 0].hist(Beige1[&quot;average_price&quot;], bins=100, range=(0,500), color = &#39;Beige&#39;) ax[1,0].set_title(&#39;Beige Shoes&#39;) ax[1,0].set_ylabel(&#39;Frequency&#39;) ax[1, 1].hist(Blue1[&quot;average_price&quot;], bins=100, range=(0,500), color = &#39;Blue&#39;) ax[1,1].set_title(&#39;Blue Shoes&#39;) ax[2, 0].hist(White1[&quot;average_price&quot;], bins=100, range=(0,500), color = &#39;white&#39;) ax[2,0].set_title(&#39;White Shoes&#39;) ax[2,0].set_xlabel(&#39;Price&#39;) ax[2,0].set_ylabel(&#39;Frequency&#39;) ax[2, 1].hist(Gray1[&quot;average_price&quot;], bins=100, range=(0,500), color = &#39;gray&#39;) ax[2,1].set_title(&#39;Gray Shoes&#39;) ax[2,1].set_xlabel(&#39;Price&#39;) plt.tight_layout()#Get rid of overlaps plt.show() #chi-squared distribution . What is the average price for the top 10 colors in decending order? . y = WomenShoe.colors.value_counts().head(10) Colors = [] for i in range (10): Colors.append([y.index[i], y[i]]) # filter the origanial dataframe and keep all the rows that has those top ten colors dfc = [] for i in range (len(Colors)): dfc.append(Colors[i][0]) color_df = WomenShoe[WomenShoe[&#39;colors&#39;].isin(dfc)] . color_df.groupby(&#39;colors&#39;)[&#39;average_price&#39;].mean().sort_values(ascending=False) . colors Black 83.681328 White 78.665582 Blue 77.766768 Brown 77.556691 Gray 71.996876 Green 68.255517 Silver 65.643744 Multi-Color 62.177169 Red 60.700265 Beige 58.799527 Name: average_price, dtype: float64 . Brand &amp; Price Analysis . Which brand have the highest price? . x = WomenShoe.brand_clean.value_counts() brands = [] for i in range (1897): if x[i] &gt; 5: brands.append([x.index[i], x[i]]) #filter the origanial dataframe and keep all the brands that have at least 5 rows in the dataset df = [] for i in range (len(brands)): df.append(brands[i][0]) WS_Brand = WomenShoe[WomenShoe[&#39;brand_clean&#39;].isin(df)] #Calculate the average price for each brand and only keep the top 20 brands df_brand = WS_Brand.groupby(&#39;brand_clean&#39;)[&#39;average_price&#39;].mean().sort_values(ascending=False).head(20) Over5 = [] for i in range (len(df_brand)): Over5.append([df_brand.index[i], df_brand[i]]) Over5_df = pd.DataFrame(Over5, columns=[&quot;brand&quot;, &quot;average_price&quot;]) . plt.style.use(&#39;ggplot&#39;) fig, ax = plt.subplots(figsize=(12, 10)) Over5_df = Over5_df.sort_values(&#39;average_price&#39;) ax.barh(Over5_df[&#39;brand&#39;], Over5_df[&#39;average_price&#39;], color= &quot;blue&quot;) ax.set_ylabel(&quot;Brands&quot;) ax.set_xlabel(&quot;Average Price&quot;) ax.set_title(&quot;Top 20 brands with Count over 5&quot;) plt.show() . brands = [] for i in range (1897): if x[i] &gt; 50: brands.append([x.index[i], x[i]]) #filter the origanial dataframe and keep all the brands that have at least 50 rows in the dataset df = [] for i in range (len(brands)): df.append(brands[i][0]) WS_Brand = WomenShoe[WomenShoe[&#39;brand_clean&#39;].isin(df)] #Calculate the average price for each brand and only keep the top 20 brand df_brand = WS_Brand.groupby(&#39;brand_clean&#39;)[&#39;average_price&#39;].mean().sort_values(ascending=False).head(20) Over5 = [] for i in range (len(df_brand)): Over5.append([df_brand.index[i], df_brand[i]]) Over5_df = pd.DataFrame(Over5, columns=[&quot;brand&quot;, &quot;average_price&quot;]) #draw a bar chart for 20 brands with the highest average price plt.style.use(&#39;ggplot&#39;) my_cmap = plt.get_cmap(&quot;viridis&quot;) fig, ax = plt.subplots(figsize=(12, 10)) Over5_df = Over5_df.sort_values(&#39;average_price&#39;) ax.barh(Over5_df[&#39;brand&#39;], Over5_df[&#39;average_price&#39;], color= my_cmap.colors ) ax.set_ylabel(&quot;Brands&quot;) ax.set_xlabel(&quot;Average Price&quot;) ax.set_title(&quot;Top 20 brands with Count over 50&quot;) plt.show() . Specific Brand &amp; Price Analysis . Which ones have the widest distribution of prices? Is there a typical price distribution across brands or within specific brands? . plt.style.use(&#39;ggplot&#39;) fig, ax = plt.subplots() ax.hist(WomenShoe[&quot;average_price&quot;], bins=50, range=(0,800), color = &#39;orange&#39;) ax.set_title(&#39;Price Distribution Across Brands&#39;) ax.set_xlabel(&quot;Price&quot;) ax.set_ylabel(&quot;Frequency&quot;) plt.show() . Ralph_lauren = WomenShoe[WomenShoe[&#39;brand_clean&#39;] == &quot;ralph lauren&quot;] Ugg = WomenShoe[WomenShoe[&#39;brand_clean&#39;] == &quot;ugg&quot;] Nike = WomenShoe[WomenShoe[&#39;brand_clean&#39;] == &quot;nike&quot;] Puma = WomenShoe[WomenShoe[&#39;brand_clean&#39;] == &quot;puma&quot;] Toms = WomenShoe[WomenShoe[&#39;brand_clean&#39;] == &quot;toms&quot;] Vans = WomenShoe[WomenShoe[&#39;brand_clean&#39;] == &quot;vans&quot;] . plt.style.use(&#39;ggplot&#39;) fig, ax = plt.subplots(3, 2, figsize=(15, 8)) ax[0, 0].hist(Ralph_lauren[&quot;average_price&quot;], bins=50, range=(0,1500), color = &#39;cadetblue&#39;) ax[0,0].set_title(&#39;Ralph Lauren&#39;) ax[0,0].set_ylabel(&quot;Frequency&quot;) ax[0, 1].hist(Ugg[&quot;average_price&quot;], bins=50, range=(0,500), color = &#39;lightblue&#39;) ax[0,1].set_title(&#39;Ugg&#39;) ax[1, 0].hist(Nike[&quot;average_price&quot;], bins=50, range=(0,500), color = &#39;skyblue&#39;) ax[1,0].set_title(&#39;Nike&#39;) ax[1,0].set_ylabel(&quot;Frequency&quot;) ax[1, 1].hist(Puma[&quot;average_price&quot;], bins=50, range=(0,500), color = &#39;steelblue&#39;) ax[1,1].set_title(&#39;Puma&#39;) ax[2, 0].hist(Toms[&quot;average_price&quot;], bins=50, range=(0,500), color = &#39;dodgerblue&#39;) ax[2,0].set_title(&#39;Toms&#39;) ax[2,0].set_ylabel(&quot;Frequency&quot;) ax[2,0].set_xlabel(&quot;Average Price&quot;) ax[2, 1].hist(Vans[&quot;average_price&quot;], bins=50, range=(0,500), color = &#39;deepskyblue&#39;) ax[2,1].set_title(&#39;Vans&#39;) ax[2,1].set_xlabel(&quot;Average Price&quot;) plt.tight_layout()#Get rid of overlaps plt.show() . Ralph_lauren[&#39;average_price&#39;].plot(kind=&#39;density&#39;, color = &#39;cadetblue&#39;) plt.title(&#39;Price Distribution for Ralph Lauren&#39;) plt.xlabel(&#39;Price&#39;) plt.show() . Ugg[&#39;average_price&#39;].plot(kind=&#39;density&#39;, color = &#39;lightblue&#39;) plt.title(&#39;Price Distribution for Ugg&#39;) plt.xlabel(&#39;Price&#39;) plt.show() . Nike[&#39;average_price&#39;].plot(kind=&#39;density&#39;, color = &#39;skyblue&#39;) plt.title(&#39;Price Distribution for Nike&#39;) plt.xlabel(&#39;Price&#39;) plt.show() . Puma[&#39;average_price&#39;].plot(kind=&#39;density&#39;, color = &#39;steelblue&#39;) plt.title(&#39;Price Distribution for Puma&#39;) plt.xlabel(&#39;Price&#39;) plt.show() . Toms[&#39;average_price&#39;].plot(kind=&#39;density&#39;, color = &#39;dodgerblue&#39;) plt.title(&#39;Price Distribution for Toms&#39;) plt.xlabel(&#39;Price&#39;) plt.show() . Vans[&#39;average_price&#39;].plot(kind=&#39;density&#39;, color = &#39;deepskyblue&#39;) plt.title(&#39;Price Distribution for Vans&#39;) plt.xlabel(&#39;Price&#39;) plt.show() . Conclusion . Overall, I explored the price distribution for shoe colors and shoe brands. The price distribution for shoe colors are more like a chi-squared distribution. The price distribution for shoe brands can be close to normal distribution. .",
            "url": "https://shudi-zhao.github.io/My_Data_Science_Blog/eda/python/data%20vidualization/2020/12/01/Women-Shoes.html",
            "relUrl": "/eda/python/data%20vidualization/2020/12/01/Women-Shoes.html",
            "date": " • Dec 1, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "Shudi Zhao . Please visit my resumefor additional information! .",
          "url": "https://shudi-zhao.github.io/My_Data_Science_Blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://shudi-zhao.github.io/My_Data_Science_Blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}
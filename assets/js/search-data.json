{
  
    
        "post0": {
            "title": "Sentiment Analysis on Amazon Reviews with Python",
            "content": "Introduction . . In this blog, I will guide you through how to perform a sentiment analysis on a text data. . We will be using the Razer_mouse_reviews.csv which I scraped from Amazon using Scrapy in Python to perform our analysis. This is only a portion of the reviews. The data contains the customer reviews for the product &quot;Razer DeathAdder Essential Gaming Mouse&quot; on Amazon and their star ratings. If you want to follow me along, you can download my dataset from here Razer_mouse_reviews.csv. You can also scrape your own dataset of any product you want on Amazon follow this tutorial: scraping amazon reviews use python scrapy. . Data cleaning . Before we start our analysis, we need to clean up our data. . Prerequisites . First, we need to import the necessary packages. Make sure you have installed all the packages below: . numpy | pandas | matplotlib | seaborn | wordcloud | sklearn | . import numpy as np import pandas as pd import seaborn as sns color = sns.color_palette() import matplotlib.pyplot as plt %matplotlib inline from wordcloud import WordCloud, STOPWORDS from sklearn.feature_extraction.text import CountVectorizer from sklearn.linear_model import LogisticRegression from sklearn.model_selection import train_test_split from sklearn.metrics import confusion_matrix,classification_report #Optional, just want to ignore the warning text in the output import warnings from pandas.core.common import SettingWithCopyWarning warnings.simplefilter(action=&quot;ignore&quot;, category=SettingWithCopyWarning) . Read the Dataframe . df1 = pd.read_csv(&quot;Razer_mouse_reviews.csv&quot;) df1.head() . From the table below, we can see the stars are strings instead of integer rating and there are so many white line in front of each comment. . stars comment . 0 | 4.0 out of 5 stars | n n n n n n n n n n n n n For an &quot;el... | . 1 | 2.0 out of 5 stars | n n n n n n n n n n n n n Every time... | . 2 | 2.0 out of 5 stars | n n n n n n n n n n n n n The unit i... | . 3 | 1.0 out of 5 stars | n n n n n n n n n n n n n O.K., Im g... | . 4 | 1.0 out of 5 stars | n n n n n n n n n n n n n The mouse ... | . Clean the Dataframe . Next, we want to replace &quot;1.0 out of 5 stars&quot; to 1, &quot;2.0 out of 5 stars&quot; to 2, &quot;3.0 out of 5 stars&quot; to 3, &quot;4.0 out of 5 stars&quot; to 4, and &quot;5.0 out of 5 stars&quot; to 5 in the stars column. Then use str.strip() to remove all the white lines at the begainning of each comment. The code is like following: . df1 =df1.replace({&quot;1.0 out of 5 stars&quot;: 1, &#39;2.0 out of 5 stars&#39;: 2, &#39;3.0 out of 5 stars&#39;: 3, &#39;4.0 out of 5 stars&#39;: 4, &#39;5.0 out of 5 stars&#39;: 5}) #Remove the new lines at the begainning of the comment df1[&#39;comment&#39;] =df1[&#39;comment&#39;].str.strip() . Good job! Your data should be look like this: . stars comment . 0 | 4 | For an &quot;elite&quot; gaming mouse with impressive fe... | . 1 | 2 | Every time my computer starts or restarts Syna... | . 2 | 2 | The unit is just built cheap. Not the quality ... | . 3 | 1 | O.K., Im going to &quot;throw&quot; this to the air. Nob... | . 4 | 1 | The mouse left click started to break within t... | . Classify Positive and Negative Comments . We will assign comments with rating four and five as 1 which means positive sentiment, rating one and two as -1 which means negative sentiment. Since, we are not interested in rating three which is the neutral sentiment in this analysis, we will drop the comments with rating three. The code below will do the job: . df = df1[df1[&#39;stars&#39;] != 3] #add one column called sentiment contains values 1 and -1 df[&#39;sentiment&#39;] = df[&#39;stars&#39;].apply(lambda rating : +1 if rating &gt; 3 else -1) #add another column sentimentt contains values negative and positive df[&#39;sentimentt&#39;] = df[&#39;sentiment&#39;].replace({-1 : &#39;negative&#39;}) df[&#39;sentimentt&#39;] = df[&#39;sentimentt&#39;].replace({1 : &#39;positive&#39;}) . Your new dataframe will look like this: . stars comment sentiment sentimentt . 0 | 4 | For an &quot;elite&quot; gaming mouse with impressive fe... | 1 | positive | . 1 | 2 | Every time my computer starts or restarts Syna... | -1 | negative | . 2 | 2 | The unit is just built cheap. Not the quality ... | -1 | negative | . 3 | 1 | O.K., Im going to &quot;throw&quot; this to the air. Nob... | -1 | negative | . 4 | 1 | The mouse left click started to break within t... | -1 | negative | . Exploratory Data Analysis . We want explore our dataset to see if there is any interesting dicoveries and findings. Althought, our dataset only has two columns, we can still do a lot of fancy graphs and vidualizations. . Stars Counts . Next, we can viduallize the number of comments in each star rating using seaborn. Make sure you use df1 to do this plot, because we deleted rating 3 in df. . #set plot theme sns.set_theme(style=&quot;darkgrid&quot;) #Specifiy the figure size plt.figure(figsize=(15,8)) ax = sns.countplot(x=&quot;stars&quot;, data = df1, palette=&quot;Blues&quot;) ax.set_title(&quot;Number of Comments in Each Rating &quot;, fontsize=20) ax.set_xlabel(&quot;Star Rating&quot;,fontsize=15) ax.set_ylabel(&quot;Number of Comments&quot;,fontsize=15) plt.show() . The result plot looks like this: . From the plot above, we can see that the majority of the customers rating is positive. On the other hand, there are almost 100 comments are rated one star and two star. Therefore, we can build a model to predict the customers rating based on their comments. We will talk about modeling in the later section. . Sentiment Counts . Now, we can take a closer look. We want to see the number of positive comments and the number of negative comments: . sns.set_theme(style=&quot;darkgrid&quot;) plt.figure(figsize=(15,8)) ax = sns.countplot(x=&quot;sentimentt&quot;, data = df, palette=&quot;coolwarm&quot;) ax.set_title(&quot;Product Sentiment&quot;, fontsize=20) ax.set_xlabel(&quot;Sentiment&quot;,fontsize=15) ax.set_ylabel(&quot;Count&quot;,fontsize=15) plt.show() . Most Frequent Words . Next, we can use the WordCloud to find the most frequent words that appeared in the comments. . # Create stopword list stopwords = set(STOPWORDS) stopwords.update([&quot;mouse&quot;, &quot;Razer&quot;, &quot;gaming&quot;]) #Joint each word in each comment together separate by space textt = &quot; &quot;.join(review for review in df.comment) wordcloud = WordCloud(stopwords=stopwords, background_color = &quot;white&quot;).generate(textt) # Plot the worldclou to show the most frequent words in the image plt.imshow(wordcloud, interpolation=&#39;bilinear&#39;) plt.axis(&quot;off&quot;) plt.show() . The result would be look like this: . From the above image, we can see the word, &quot;great&quot;, &quot;feel&quot;, &quot;use&quot;, &quot;button&quot;, &quot;one&quot;, &quot;software&quot; are the most frequent words. Next, we can also find the most frequent words in the positive and negative comments by split the comments into positive and negative comments. . Most Frequent Words in the Positive Comments . positive = df[df[&#39;sentiment&#39;] == 1] stopwords = set(STOPWORDS) stopwords.update([&quot;mouse&quot;, &quot;Razer&quot;, &quot;gaming&quot;, &quot;use&quot;, &quot;button&quot;]) ## good and great removed because they were included in negative sentiment pos = &quot; &quot;.join(review for review in positive.comment) wordcloud2 = WordCloud(stopwords=stopwords, background_color = &quot;white&quot;).generate(pos) plt.imshow(wordcloud2, interpolation=&#39;bilinear&#39;) plt.axis(&quot;off&quot;) plt.show() . Most Frequent Words in the Nagetive Comments . negative = df[df[&#39;sentiment&#39;] == -1] negative = negative.dropna() neg = &quot; &quot;.join(review for review in negative.comment) wordcloud3 = WordCloud(stopwords=stopwords, background_color = &quot;white&quot;).generate(neg) plt.imshow(wordcloud3, interpolation=&#39;bilinear&#39;) plt.axis(&quot;off&quot;) plt.show() . Data Modeling . Finally, it comes to the most exciting part, data modeling. We want to use logistic regression to predict whether the comments is positive or negative. Before doing so, we have to do some preprocessing. . Preprocessing . Step one: Remove punctuations . You can use the following function to remove puctuations in the comments: . def remove_punctuation(text): final = &quot;&quot;.join(u for u in text if u not in (&quot;?&quot;, &quot;.&quot;, &quot;;&quot;, &quot;:&quot;, &quot;!&quot;,&#39;&quot;&#39;)) return final df[&#39;comment&#39;] = df[&#39;comment&#39;].apply(remove_punctuation) . Your new dataframe will like following table: . stars comment sentiment sentimentt . 0 | 4 | For an elite gaming mouse with impressive feat... | 1 | positive | . 1 | 2 | Every time my computer starts or restarts Syna... | -1 | negative | . 2 | 2 | The unit is just built cheap Not the quality p... | -1 | negative | . 3 | 1 | OK, Im going to throw this to the air Nobody a... | -1 | negative | . 4 | 1 | The mouse left click started to break within t... | -1 | negative | . Step two: Select the Feature for Modeling . In this example, our model only take two teatures. We will use comment to predict sentiment, 1 for positive, -1 for negative. . dfNew = df[[&#39;comment&#39;,&#39;sentiment&#39;]] dfNew.head() . Your selected features are following columns: . comment sentiment . 0 | For an elite gaming mouse with impressive feat... | 1 | . 1 | Every time my computer starts or restarts Syna... | -1 | . 2 | The unit is just built cheap Not the quality p... | -1 | . 3 | OK, Im going to throw this to the air Nobody a... | -1 | . 4 | The mouse left click started to break within t... | -1 | . Step three: Split Train and Test Data . We can randomly split our data using the train_test_split function in the sklearn package, which already imported in the very begining. 80% of our data will be used for training, and 20% will be used for testing. Thre are many other methods to split your data, feel free to use your own way. . train ,test = train_test_split(df,test_size=0.2) . Step four: Vectorize Comments . In this step, we want to Convert the collection of text comments to a matrix of token counts, because the logistic regression algorithm cannot understand text. . We will use a count vectorizer from the Scikit-learn library to transform the text comments into a bag of words model, which will find the unique words in each comment, and count the occurence for each word in each comment. . vectorizer = CountVectorizer(token_pattern=r&#39; b w+ b&#39;) #vectorize both train data and test data train_matrix = vectorizer.fit_transform(train[&#39;comment&#39;]) test_matrix = vectorizer.transform(test[&#39;comment&#39;]) . Logistic Regression . Finally, we can use the Logistic Regression from the Scikit-learn library to fit our trainning data and make predictions using our test data. . lr = LogisticRegression() #Split target and independent variables X_train = train_matrix X_test = test_matrix y_train = train[&#39;sentiment&#39;] y_test = test[&#39;sentiment&#39;] #Fit model on data lr.fit(X_train,y_train) #Make predictions predictions = lr.predict(X_test) . Data Validation . We can test our model accuracy use the confusion matrix, which imported from the sklearn package in the very beginning: . new = np.asarray(y_test) cf_matrix = confusion_matrix(predictions,y_test) #Display our confusion matrix in a heatmap: group_names = [&quot;True Negative&quot;,&quot;False Positive&quot;,&quot;False Negative&quot;,&quot;True Positive&quot;] group_counts = [&quot;{0:0.0f}&quot;.format(value) for value in cf_matrix.flatten()] group_percentages = [&quot;{0:.2%}&quot;.format(value) for value in cf_matrix.flatten()/np.sum(cf_matrix)] labels = [f&quot;{v1} n{v2} n{v3}&quot; for v1, v2, v3 in zip(group_names,group_counts,group_percentages)] labels = np.asarray(labels).reshape(2,2) sns.heatmap(cf_matrix, annot=labels, fmt=&#39;&#39;, cmap=&#39;Blues&#39;) plt.show() . We can also generate a classification report to validate our model accuracy: . print(classification_report(predictions,y_test)) . precision recall f1-score support -1 0.67 0.86 0.75 14 1 0.97 0.91 0.94 70 accuracy 0.90 84 macro avg 0.82 0.89 0.85 84 weighted avg 0.92 0.90 0.91 84 . The overall accuracy of the model on the test data is around 90%, which is pretty good since our dataset is not very large. . Thank you for reading it, I hope this tutorial will help you to understand the basics of the sentiment analysis, WorldCloud, and Logistic Regression. If you have any questions feel free to comment below. Good luck everyone, you are on the right track. .",
            "url": "https://shudi-zhao.github.io/My_Data_Science_Blog/sentiment%20analysis/wordcloud/python/2021/03/14/Sentiment-Analysis.html",
            "relUrl": "/sentiment%20analysis/wordcloud/python/2021/03/14/Sentiment-Analysis.html",
            "date": " • Mar 14, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master - badges: true - comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . place a #collapse-output flag at the top of any cell if you want to put the output under a collapsable element that is closed by default, but give the reader the option to open it: . print(&#39;The comment #collapse-output was used to collapse the output of this cell by default but you can expand it.&#39;) . The comment #collapse-output was used to collapse the output of this cell by default but you can expand it. . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://shudi-zhao.github.io/My_Data_Science_Blog/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://shudi-zhao.github.io/My_Data_Science_Blog/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://shudi-zhao.github.io/My_Data_Science_Blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://shudi-zhao.github.io/My_Data_Science_Blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}
{
  
    
        "post0": {
            "title": "Stock Price Prediction using Python",
            "content": "Introduction . In this tutorial, I will guide you throught how to predict the stock price of the most active stock based on the historical data. . Before we start, let me give you some basic ideal. First, we are going to use selenim for web scraping. We need to get the name of the most active stock of the day on yahoo finance. Selenium is a hot tool not only in web scraping but also in automated web application test. If you haven&#39;t use selenium before, I recomend you to watch this fantanstic youtube selenium tutorial to set up selenium. And use this selenium python tutorial as more detailed reference. Then, we will get the historical data of that most active stock. Next, we will perform some prediction tasks using machine simple learning models. Finally, we can send our predictions to our clients by email. . . Prerequisites . First, we need to import the necessary packages. Make sure you have installed all the packages below: . numpy | pandas | matplotlib | sklearn | iexfinance | selenium | . import numpy as np from datetime import datetime import smtplib from selenium import webdriver import os import pandas as pd #For Prediction from sklearn.linear_model import LinearRegression from sklearn import preprocessing#,cross_validation this is no longer aviliable from sklearn.model_selection import train_test_split #use this instead #For Stock Data from iexfinance.stocks import get_historical_data from iexfinance.refdata import get_symbols import matplotlib.pyplot as plt import copy . Get Stock Name . First, we need to create our chrome drive then use driver.get(url) navigate to our desired webpage: https://finance.yahoo.com/most-active which will display the top 25 most active stocks in this page. If you are interested in other stocks you can change this link to the URL you want. Inside webdriver.Chrome() you will need to type your chromedriver path. . driver = webdriver.Chrome( &#39;Type the directory of your chromedriver here&#39;) url = &quot;https://finance.yahoo.com/most-active&quot; driver.get(url) . Next, we want to find the xpath of the most active stock name. You can follow the following steps to get the xpath: . First, go to your desired webpage and inspect the element of that webpage. . Click the &quot;Select an Element&quot; button: . . Click on the first ticker: . . Next, copy the xpath following the following instruction: . . After you find the xpath, you can get the element use the code below: . ticker = driver.find_element_by_xpath( &#39;//*[@id=&quot;scr-res-table&quot;]/div[1]/table/tbody/tr[1]/td[1]/a&#39;) . Last, we want to get the stock name by calling ticker.text, and make a deep copy, because we will lose the stock name after we call driver.quit(). . stock = copy.deepcopy(ticker.text) driver.quit() . Finally, we successfully scraped the stock name of the most active stock using selenium. Below is the name we got from the yahoo finance: . stock . &#39;SNDL&#39; . Get Historical Data . Now, we can get the historical data for the most active stock. Inside the get_historical_data(), you will need to set the start and end date, the output format(we will use pandas in this project). . For the token, you will need to visit iexcloud to create an account to get your API token. You can choose the free version, but it only offers you a very limided access. . Last, we will save the historical data into a csv file. . start = datetime(2020, 3, 1) end = datetime(2021, 3, 1) #Download Historical stock data df = get_historical_data(stock, start, end, output_format=&#39;pandas&#39;, token=&quot;pk_422a359c341b427ea05864740c233fe3&quot;) csv_name = ( stock + &#39;.csv&#39;) df.to_csv(csv_name) . You dataframe should look like this: . Unnamed: 0 close high low open symbol volume id key subkey ... uLow uVolume fOpen fClose fHigh fLow fVolume label change changePercent . 0 | 2020-03-02 | 1.48 | 1.540 | 1.40 | 1.44 | SNDL | 1056600 | HISTORICAL_PRICES | SNDL | NaN | ... | 1.40 | 1056600 | 1.44 | 1.48 | 1.540 | 1.40 | 1056600 | Mar 2, 20 | 0.06 | 0.0423 | . 1 | 2020-03-03 | 1.48 | 1.500 | 1.35 | 1.45 | SNDL | 945902 | HISTORICAL_PRICES | SNDL | NaN | ... | 1.35 | 945902 | 1.45 | 1.48 | 1.500 | 1.35 | 945902 | Mar 3, 20 | 0.00 | 0.0000 | . 2 | 2020-03-04 | 1.65 | 1.700 | 1.47 | 1.50 | SNDL | 1522873 | HISTORICAL_PRICES | SNDL | NaN | ... | 1.47 | 1522873 | 1.50 | 1.65 | 1.700 | 1.47 | 1522873 | Mar 4, 20 | 0.17 | 0.1149 | . 3 | 2020-03-05 | 1.45 | 1.650 | 1.44 | 1.60 | SNDL | 673430 | HISTORICAL_PRICES | SNDL | NaN | ... | 1.44 | 673430 | 1.60 | 1.45 | 1.650 | 1.44 | 673430 | Mar 5, 20 | -0.20 | -0.1212 | . 4 | 2020-03-06 | 1.33 | 1.475 | 1.33 | 1.45 | SNDL | 494977 | HISTORICAL_PRICES | SNDL | NaN | ... | 1.33 | 494977 | 1.45 | 1.33 | 1.475 | 1.33 | 494977 | Mar 6, 20 | -0.12 | -0.0828 | . ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 247 | 2021-02-23 | 1.26 | 1.330 | 1.10 | 1.29 | SNDL | 397249358 | HISTORICAL_PRICES | SNDL | NaN | ... | 1.10 | 397249358 | 1.29 | 1.26 | 1.330 | 1.10 | 397249358 | Feb 23, 21 | -0.17 | -0.1189 | . 248 | 2021-02-24 | 1.45 | 1.470 | 1.28 | 1.32 | SNDL | 433296256 | HISTORICAL_PRICES | SNDL | NaN | ... | 1.28 | 433296256 | 1.32 | 1.45 | 1.470 | 1.28 | 433296256 | Feb 24, 21 | 0.19 | 0.1508 | . 249 | 2021-02-25 | 1.37 | 1.640 | 1.36 | 1.54 | SNDL | 391487356 | HISTORICAL_PRICES | SNDL | NaN | ... | 1.36 | 391487356 | 1.54 | 1.37 | 1.640 | 1.36 | 391487356 | Feb 25, 21 | -0.08 | -0.0552 | . 250 | 2021-02-26 | 1.33 | 1.490 | 1.31 | 1.39 | SNDL | 255416545 | HISTORICAL_PRICES | SNDL | NaN | ... | 1.31 | 255416545 | 1.39 | 1.33 | 1.490 | 1.31 | 255416545 | Feb 26, 21 | -0.04 | -0.0292 | . 251 | 2021-03-01 | 1.35 | 1.440 | 1.33 | 1.41 | SNDL | 237436937 | HISTORICAL_PRICES | SNDL | NaN | ... | 1.33 | 237436937 | 1.41 | 1.35 | 1.440 | 1.33 | 237436937 | Mar 1, 21 | 0.02 | 0.0150 | . 252 rows × 26 columns . Modeling . Before modeling, we need to clean our data a little bit. First, we will select the useful features, because there are too many columns in this dataframe. Then, add the prediction column. . #read the data data = pd.read_csv(csv_name) #feature selection df = data[[&#39;close&#39;, &#39;high&#39;, &#39;low&#39;, &#39;open&#39;,&#39;volume&#39;, &#39;change&#39;]] #add a prediction column (eg: today&#39;s prediction is the close price of tomorrow) df[&#39;prediction&#39;] = df[&#39;close&#39;].shift(-1) #drop the last row, because the value in the prediction column is nan df.dropna(inplace=True) . Your new data frame will look like this: . close high low open volume change prediction . 0 | 1.48 | 1.540 | 1.40 | 1.440 | 1056600 | 0.06 | 1.48 | . 1 | 1.48 | 1.500 | 1.35 | 1.450 | 945902 | 0.00 | 1.65 | . 2 | 1.65 | 1.700 | 1.47 | 1.500 | 1522873 | 0.17 | 1.45 | . 3 | 1.45 | 1.650 | 1.44 | 1.600 | 673430 | -0.20 | 1.33 | . 4 | 1.33 | 1.475 | 1.33 | 1.450 | 494977 | -0.12 | 1.22 | . ... | ... | ... | ... | ... | ... | ... | ... | . 246 | 1.43 | 1.600 | 1.40 | 1.425 | 255266388 | -0.10 | 1.26 | . 247 | 1.26 | 1.330 | 1.10 | 1.290 | 397249358 | -0.17 | 1.45 | . 248 | 1.45 | 1.470 | 1.28 | 1.320 | 433296256 | 0.19 | 1.37 | . 249 | 1.37 | 1.640 | 1.36 | 1.540 | 391487356 | -0.08 | 1.33 | . 250 | 1.33 | 1.490 | 1.31 | 1.390 | 255416545 | -0.04 | 1.35 | . 251 rows × 7 columns . Next, we will built our regression model, a detailed explaination is commented below: . #X is the predictor variable, Y is the target variable X = np.array(df.drop([&#39;prediction&#39;], 1)) Y = np.array(df[&#39;prediction&#39;]) #Nomalize our predictor variables X = preprocessing.scale(X) #the last row in the predictor variable X_prediction = X[-1:] #the last row in the target variable Y_ans = Y[-1:] #Delete the last row in X and Y, because we don&#39;t want it to be in the train data. X = X[:-1] Y = Y[:-1] #Split our data into train and test data X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2) #Performing the Regression on the training data clf = LinearRegression() clf.fit(X_train, Y_train) #Predict the closing price of X_prediction prediction = (clf.predict(X_prediction)) #Accuracy score of our model result = clf.score(X_test, Y_test) . Last, we can write a function to send result to our clients. The smtplib module allowed up to send emails to any internet machine with an SMTP. Check more details for SMTP. . def sendMessage(text): # If you&#39;re using Gmail to send the message, you might need to # go into the security settings of your email account and # enable the &quot;Allow less secure apps&quot; option username = &quot;Your Email Address&quot; password = &quot;The password of your email&quot; to = &quot;the email you will sent to ...@gmail.com&quot; message = text Subject = &quot;Stock Prediction&quot; msg = &#39;Subject: {} n n{}&#39;.format(Subject, text) server = smtplib.SMTP(&#39;smtp.gmail.com&#39;, 587) server.starttls() server.login(username, password) server.sendmail(username, to, msg) server.quit() print(&#39;Sent&#39;) output = (&quot; n nStock: &quot; + str(stock) + &quot; nClose Price on &quot; + str(data.loc[data.index[[-2]], &#39;Unnamed: 0&#39;].item()) + &quot;: $&quot; + str(data.loc[data.index[[-2]].item(), &#39;close&#39;]) + &quot; nPrediction for the next day closing: $%.2f&quot; % (prediction[0]) + &quot; nActuall closing: $%.2f&quot; % (Y_ans[0]) + &quot; nModel Accuracy: %.2f%%&quot; % (result*100.0)) sendMessage(output) . The email will be in this format: . . Final Application . Finally, we can add everything up to a complete application. The following code will do the job: . # Final application def getStocks(): #Navigating to the Yahoo stock screener driver = webdriver.Chrome( &#39;Type the directory of your chromedriver here&#39;) url = &quot;https://finance.yahoo.com/most-active&quot; driver.get(url) #Creating a stock list and iterating through the ticker names on the stock screener list #Xpath: //*[@id=&quot;scr-res-table&quot;]/div[1]/table/tbody/tr[1]/td[1]/a ticker = driver.find_element_by_xpath( &#39;//*[@id=&quot;scr-res-table&quot;]/div[1]/table/tbody/tr[1]/td[1]/a&#39;) #stock = ticker.text stock = copy.deepcopy(ticker.text) driver.quit() return predictData(stock) def sendMessage(text): # If you&#39;re using Gmail to send the message, you might need to # go into the security settings of your email account and # enable the &quot;Allow less secure apps&quot; option username = &quot;Your Email Address&quot; password = &quot;The password of your email&quot; to = &quot;the email you will sent to ...@gmail.com&quot; message = text Subject = &quot;Stock Prediction&quot; msg = &#39;Subject: {} n n{}&#39;.format(Subject, text) server = smtplib.SMTP(&#39;smtp.gmail.com&#39;, 587) server.starttls() server.login(username, password) server.sendmail(username, to, msg) server.quit() print(&#39;Sent&#39;) def predictData(stock): start = datetime(2018, 10, 2) end = datetime(2019, 10, 2) df = get_historical_data(stock, start, end, output_format=&#39;pandas&#39;, token=&quot;pk_422a359c341b427ea05864740c233fe3&quot;) csv_name = ( stock + &#39;.csv&#39;) df.to_csv(csv_name) data = pd.read_csv(csv_name) df = data[[&#39;close&#39;, &#39;high&#39;, &#39;low&#39;, &#39;open&#39;,&#39;volume&#39;, &#39;change&#39;]] df[&#39;prediction&#39;] = df[&#39;close&#39;].shift(-1) df.dropna(inplace=True) #Predicting the Stock price in the future X = np.array(df.drop([&#39;prediction&#39;], 1)) Y = np.array(df[&#39;prediction&#39;]) X = preprocessing.scale(X) X_prediction = X[-1:] Y_ans = Y[-1:] X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2) #Performing the Regression on the training data clf = LinearRegression() clf.fit(X_train, Y_train) prediction = (clf.predict(X_prediction)) result = clf.score(X_train, Y_train) output = (&quot; n nStock: &quot; + str(stock) + &quot; nClose Price on &quot; + str(data.loc[data.index[[-2]], &#39;Unnamed: 0&#39;].item()) + &quot;: $&quot; + str(data.loc[data.index[[-2]].item(), &#39;close&#39;]) + &quot; nPrediction for tomorrow closing: $%.2f&quot; % (prediction[0]) + &quot; nActuall closing: $%.2f&quot; % (Y_ans[0]) + &quot; nModel Accuracy: %.2f%%&quot; % (result*100.0)) sendMessage(output) if __name__ == &#39;__main__&#39;: getStocks() .",
            "url": "https://shudi-zhao.github.io/My_Data_Science_Blog/machine%20learning/python/selenium/stock/2021/03/17/Stock-Prediction.html",
            "relUrl": "/machine%20learning/python/selenium/stock/2021/03/17/Stock-Prediction.html",
            "date": " • Mar 17, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Sentiment Analysis on Amazon Reviews with Python",
            "content": "Introduction . . In this blog, I will guide you through how to perform a sentiment analysis on a text data. . We will be using the Razer_mouse_reviews.csv which I scraped from Amazon using Scrapy in Python to perform our analysis. This is only a portion of the reviews. The data contains the customer reviews for the product &quot;Razer DeathAdder Essential Gaming Mouse&quot; on Amazon and their star ratings. If you want to follow me along, you can download my dataset from here Razer_mouse_reviews.csv. You can also scrape your own dataset of any product you want on Amazon follow this tutorial: scraping amazon reviews use python scrapy. . Data cleaning . Before we start our analysis, we need to clean up our data. . Prerequisites . First, we need to import the necessary packages. Make sure you have installed all the packages below: . numpy | pandas | matplotlib | seaborn | wordcloud | sklearn | . import numpy as np import pandas as pd import seaborn as sns color = sns.color_palette() import matplotlib.pyplot as plt %matplotlib inline from wordcloud import WordCloud, STOPWORDS from sklearn.feature_extraction.text import CountVectorizer from sklearn.linear_model import LogisticRegression from sklearn.model_selection import train_test_split from sklearn.metrics import confusion_matrix,classification_report #Optional, just want to ignore the warning text in the output import warnings from pandas.core.common import SettingWithCopyWarning warnings.simplefilter(action=&quot;ignore&quot;, category=SettingWithCopyWarning) . Read the Dataframe . df1 = pd.read_csv(&quot;Razer_mouse_reviews.csv&quot;) df1.head() . From the table below, we can see the stars are strings instead of integer rating and there are so many white line in front of each comment. . stars comment . 0 | 4.0 out of 5 stars | n n n n n n n n n n n n n For an &quot;el... | . 1 | 2.0 out of 5 stars | n n n n n n n n n n n n n Every time... | . 2 | 2.0 out of 5 stars | n n n n n n n n n n n n n The unit i... | . 3 | 1.0 out of 5 stars | n n n n n n n n n n n n n O.K., Im g... | . 4 | 1.0 out of 5 stars | n n n n n n n n n n n n n The mouse ... | . Clean the Dataframe . Next, we want to replace &quot;1.0 out of 5 stars&quot; to 1, &quot;2.0 out of 5 stars&quot; to 2, &quot;3.0 out of 5 stars&quot; to 3, &quot;4.0 out of 5 stars&quot; to 4, and &quot;5.0 out of 5 stars&quot; to 5 in the stars column. Then use str.strip() to remove all the white lines at the begainning of each comment. The code is like following: . df1 =df1.replace({&quot;1.0 out of 5 stars&quot;: 1, &#39;2.0 out of 5 stars&#39;: 2, &#39;3.0 out of 5 stars&#39;: 3, &#39;4.0 out of 5 stars&#39;: 4, &#39;5.0 out of 5 stars&#39;: 5}) #Remove the new lines at the begainning of the comment df1[&#39;comment&#39;] =df1[&#39;comment&#39;].str.strip() . Good job! Your data should be look like this: . stars comment . 0 | 4 | For an &quot;elite&quot; gaming mouse with impressive fe... | . 1 | 2 | Every time my computer starts or restarts Syna... | . 2 | 2 | The unit is just built cheap. Not the quality ... | . 3 | 1 | O.K., Im going to &quot;throw&quot; this to the air. Nob... | . 4 | 1 | The mouse left click started to break within t... | . Classify Positive and Negative Comments . We will assign comments with rating four and five as 1 which means positive sentiment, rating one and two as -1 which means negative sentiment. Since, we are not interested in rating three which is the neutral sentiment in this analysis, we will drop the comments with rating three. The code below will do the job: . df = df1[df1[&#39;stars&#39;] != 3] #add one column called sentiment contains values 1 and -1 df[&#39;sentiment&#39;] = df[&#39;stars&#39;].apply(lambda rating : +1 if rating &gt; 3 else -1) #add another column sentimentt contains values negative and positive df[&#39;sentimentt&#39;] = df[&#39;sentiment&#39;].replace({-1 : &#39;negative&#39;}) df[&#39;sentimentt&#39;] = df[&#39;sentimentt&#39;].replace({1 : &#39;positive&#39;}) . Your new dataframe will look like this: . stars comment sentiment sentimentt . 0 | 4 | For an &quot;elite&quot; gaming mouse with impressive fe... | 1 | positive | . 1 | 2 | Every time my computer starts or restarts Syna... | -1 | negative | . 2 | 2 | The unit is just built cheap. Not the quality ... | -1 | negative | . 3 | 1 | O.K., Im going to &quot;throw&quot; this to the air. Nob... | -1 | negative | . 4 | 1 | The mouse left click started to break within t... | -1 | negative | . Exploratory Data Analysis . We want explore our dataset to see if there is any interesting dicoveries and findings. Althought, our dataset only has two columns, we can still do a lot of fancy graphs and vidualizations. . Stars Counts . Next, we can viduallize the number of comments in each star rating using seaborn. Make sure you use df1 to do this plot, because we deleted rating 3 in df. . #set plot theme sns.set_theme(style=&quot;darkgrid&quot;) #Specifiy the figure size plt.figure(figsize=(15,8)) ax = sns.countplot(x=&quot;stars&quot;, data = df1, palette=&quot;Blues&quot;) ax.set_title(&quot;Number of Comments in Each Rating &quot;, fontsize=20) ax.set_xlabel(&quot;Star Rating&quot;,fontsize=15) ax.set_ylabel(&quot;Number of Comments&quot;,fontsize=15) plt.show() . The result plot looks like this: . From the plot above, we can see that the majority of the customers rating is positive. On the other hand, there are almost 100 comments are rated one star and two star. Therefore, we can build a model to predict the customers rating based on their comments. We will talk about modeling in the later section. . Sentiment Counts . Now, we can take a closer look. We want to see the number of positive comments and the number of negative comments: . sns.set_theme(style=&quot;darkgrid&quot;) plt.figure(figsize=(15,8)) ax = sns.countplot(x=&quot;sentimentt&quot;, data = df, palette=&quot;coolwarm&quot;) ax.set_title(&quot;Product Sentiment&quot;, fontsize=20) ax.set_xlabel(&quot;Sentiment&quot;,fontsize=15) ax.set_ylabel(&quot;Count&quot;,fontsize=15) plt.show() . Most Frequent Words . Next, we can use the WordCloud to find the most frequent words that appeared in the comments. . # Create stopword list stopwords = set(STOPWORDS) stopwords.update([&quot;mouse&quot;, &quot;Razer&quot;, &quot;gaming&quot;]) #Joint each word in each comment together separate by space textt = &quot; &quot;.join(review for review in df.comment) wordcloud = WordCloud(stopwords=stopwords, background_color = &quot;white&quot;).generate(textt) # Plot the worldclou to show the most frequent words in the image plt.imshow(wordcloud, interpolation=&#39;bilinear&#39;) plt.axis(&quot;off&quot;) plt.show() . The result would be look like this: . From the above image, we can see the word, &quot;great&quot;, &quot;feel&quot;, &quot;use&quot;, &quot;button&quot;, &quot;one&quot;, &quot;software&quot; are the most frequent words. Next, we can also find the most frequent words in the positive and negative comments by split the comments into positive and negative comments. . Most Frequent Words in the Positive Comments . positive = df[df[&#39;sentiment&#39;] == 1] stopwords = set(STOPWORDS) stopwords.update([&quot;mouse&quot;, &quot;Razer&quot;, &quot;gaming&quot;, &quot;use&quot;, &quot;button&quot;]) ## good and great removed because they were included in negative sentiment pos = &quot; &quot;.join(review for review in positive.comment) wordcloud2 = WordCloud(stopwords=stopwords, background_color = &quot;white&quot;).generate(pos) plt.imshow(wordcloud2, interpolation=&#39;bilinear&#39;) plt.axis(&quot;off&quot;) plt.show() . Most Frequent Words in the Nagetive Comments . negative = df[df[&#39;sentiment&#39;] == -1] negative = negative.dropna() neg = &quot; &quot;.join(review for review in negative.comment) wordcloud3 = WordCloud(stopwords=stopwords, background_color = &quot;white&quot;).generate(neg) plt.imshow(wordcloud3, interpolation=&#39;bilinear&#39;) plt.axis(&quot;off&quot;) plt.show() . Data Modeling . Finally, it comes to the most exciting part, data modeling. We want to use logistic regression to predict whether the comments is positive or negative. Before doing so, we have to do some preprocessing. . Preprocessing . Step one: Remove punctuations . You can use the following function to remove puctuations in the comments: . def remove_punctuation(text): final = &quot;&quot;.join(u for u in text if u not in (&quot;?&quot;, &quot;.&quot;, &quot;;&quot;, &quot;:&quot;, &quot;!&quot;,&#39;&quot;&#39;)) return final df[&#39;comment&#39;] = df[&#39;comment&#39;].apply(remove_punctuation) . Your new dataframe will like following table: . stars comment sentiment sentimentt . 0 | 4 | For an elite gaming mouse with impressive feat... | 1 | positive | . 1 | 2 | Every time my computer starts or restarts Syna... | -1 | negative | . 2 | 2 | The unit is just built cheap Not the quality p... | -1 | negative | . 3 | 1 | OK, Im going to throw this to the air Nobody a... | -1 | negative | . 4 | 1 | The mouse left click started to break within t... | -1 | negative | . Step two: Select the Feature for Modeling . In this example, our model only take two teatures. We will use comment to predict sentiment, 1 for positive, -1 for negative. . dfNew = df[[&#39;comment&#39;,&#39;sentiment&#39;]] dfNew.head() . Your selected features are following columns: . comment sentiment . 0 | For an elite gaming mouse with impressive feat... | 1 | . 1 | Every time my computer starts or restarts Syna... | -1 | . 2 | The unit is just built cheap Not the quality p... | -1 | . 3 | OK, Im going to throw this to the air Nobody a... | -1 | . 4 | The mouse left click started to break within t... | -1 | . Step three: Split Train and Test Data . We can randomly split our data using the train_test_split function in the sklearn package, which already imported in the very begining. 80% of our data will be used for training, and 20% will be used for testing. Thre are many other methods to split your data, feel free to use your own way. . train ,test = train_test_split(df,test_size=0.2) . Step four: Vectorize Comments . In this step, we want to Convert the collection of text comments to a matrix of token counts, because the logistic regression algorithm cannot understand text. . We will use a count vectorizer from the Scikit-learn library to transform the text comments into a bag of words model, which will find the unique words in each comment, and count the occurence for each word in each comment. . vectorizer = CountVectorizer(token_pattern=r&#39; b w+ b&#39;) #vectorize both train data and test data train_matrix = vectorizer.fit_transform(train[&#39;comment&#39;]) test_matrix = vectorizer.transform(test[&#39;comment&#39;]) . Logistic Regression . Finally, we can use the Logistic Regression from the Scikit-learn library to fit our trainning data and make predictions using our test data. . lr = LogisticRegression() #Split target and independent variables X_train = train_matrix X_test = test_matrix y_train = train[&#39;sentiment&#39;] y_test = test[&#39;sentiment&#39;] #Fit model on data lr.fit(X_train,y_train) #Make predictions predictions = lr.predict(X_test) . Data Validation . We can test our model accuracy use the confusion matrix, which imported from the sklearn package in the very beginning: . new = np.asarray(y_test) cf_matrix = confusion_matrix(predictions,y_test) #Display our confusion matrix in a heatmap: group_names = [&quot;True Negative&quot;,&quot;False Positive&quot;,&quot;False Negative&quot;,&quot;True Positive&quot;] group_counts = [&quot;{0:0.0f}&quot;.format(value) for value in cf_matrix.flatten()] group_percentages = [&quot;{0:.2%}&quot;.format(value) for value in cf_matrix.flatten()/np.sum(cf_matrix)] labels = [f&quot;{v1} n{v2} n{v3}&quot; for v1, v2, v3 in zip(group_names,group_counts,group_percentages)] labels = np.asarray(labels).reshape(2,2) sns.heatmap(cf_matrix, annot=labels, fmt=&#39;&#39;, cmap=&#39;Blues&#39;) plt.show() . We can also generate a classification report to validate our model accuracy: . print(classification_report(predictions,y_test)) . precision recall f1-score support -1 0.67 0.86 0.75 14 1 0.97 0.91 0.94 70 accuracy 0.90 84 macro avg 0.82 0.89 0.85 84 weighted avg 0.92 0.90 0.91 84 . The overall accuracy of the model on the test data is around 90%, which is pretty good since our dataset is not very large. . Thank you for reading it, I hope this tutorial will help you to understand the basics of the sentiment analysis, WorldCloud, and Logistic Regression. If you have any questions feel free to comment below. Good luck everyone, you are on the right track. .",
            "url": "https://shudi-zhao.github.io/My_Data_Science_Blog/sentiment%20analysis/wordcloud/python/machine%20learning/data%20mining/2021/03/14/Sentiment-Analysis.html",
            "relUrl": "/sentiment%20analysis/wordcloud/python/machine%20learning/data%20mining/2021/03/14/Sentiment-Analysis.html",
            "date": " • Mar 14, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "My Resume",
            "content": "EDUCATION . Fei Tian College—Middletown, NY Graduating: May 2022 . Bachelor of Science in Data Science Cumulative GPA: 3.93/4.00 . Two-time winner of the Provost’s Outstanding Student Award . | Three-time winner of the Academic Excellence Award . | . Thomas Kelly High School, Chicago, IL June 2018 . Received: The Seal of Biliteracy—English and Mandarin Unweighted GPA: 3.90/4.00 . COURSES . Data Mining, Statistical Computing and Graphics, Database, Cloud Computing and Big Data, Data Structure Algorithms, Linear Regression, Statistical Theory and Method, Business Data Analytics, Microeconomics . PROJECTS . Data Science Project with AWS EC2 and RDS Feb 2020–Aug 2020 . Created a comprehensive system flight booking system with a MySQL database backend to store user login, flight information, and booking data . | Used Python flask and HTML, ran on Amazon EC2, created a user-friendly front-end UI . | . Coronavirus Tracker (Tableau) May 2020–Aug 2020 . Developed an interactive animated Tableau time series analysis dashboard with waterfall charts, line charts, and maps to visualize COVID-19 spreads within the U.S. and globally . | Data manipulation in Excel using advanced formulas (VLOOKUP, Index match, pivot table) . | . Data Analysis in R Project Lead Sep 2019–Dec 2019 . Spearheaded linear regression to forecast SAT score and correlation analysis using the time spent studying and GPA . | Performed data manipulation, data cleaning, normalization, and prescriptive analytics . | . EXPERIENCE . Arc’teryx, Central Valley, NY May 2019–Present . Product Guide . Results-oriented and increase 30% sales revenue and 35% customer service by leveraging my communication, bilingual literacy and by analyzing foot traffic at the mall  | . Managing large amounts of incoming calls and follow company escalation policy . | Identifying causes of customer problems and complaints with the best solution . | . SKILLS . Advanced programming skills in MySQL, Python, Java, and HTML . | Data analysis and visualization in R ggplot, Tableau, Python Matplotlib and Pandas . | Advanced Proficiency in Microsoft Office, Excel, PowerPoint and Word. . | Strong negotiation, financial analytical skill, qualitative and quantitative research skills . | Project management, public relations, teamwork, detail oriented, professionalism, social skill . | .",
            "url": "https://shudi-zhao.github.io/My_Data_Science_Blog/2021/03/10/My-Resume.html",
            "relUrl": "/2021/03/10/My-Resume.html",
            "date": " • Mar 10, 2021"
        }
        
    
  
    
        ,"post3": {
            "title": "Women Shoe Prices",
            "content": "Women&#39;s Shoe Prices . Introduction . This is a list of 10,000 women&#39;s shoes and their product information provided by Datafiniti&#39;s Product Database. . The dataset includes shoe name, brand, price, and more. Each shoe will have an entry for each price found for it and some shoes may have multiple entries. . Import libraries . import numpy as np import pandas as pd import matplotlib.pyplot as plt %matplotlib inline . Loading Data . WomenShoe = pd.read_csv(&#39;Women_Shoe_Price.csv&#39;, parse_dates = [&#39;dateAdded&#39;, &#39;dateUpdated&#39;, &#39;prices.dateAdded&#39;, &#39;prices.dateSeen&#39;] ) . WomenShoe.shape . (19034, 47) . Data Cleaning . Missing Values . miss_val = (WomenShoe.isnull().sum()/len(WomenShoe)*100).sort_values(ascending=False) miss_val[miss_val&gt;0] . count 100.000000 flavors 100.000000 isbn 100.000000 websiteIDs 100.000000 quantities 100.000000 prices.count 100.000000 prices.source 100.000000 prices.flavor 100.000000 prices.warranty 99.926447 prices.availability 99.338027 prices.size 97.336345 prices.color 97.157718 prices.returnPolicy 96.075444 weight 95.649890 reviews 94.672691 asins 88.410213 dimension 87.296417 prices.shipping 76.037617 prices.offer 64.038037 skus 62.782389 sizes 56.567196 manufacturer 54.555007 ean 48.450142 upc 43.963434 descriptions 43.411789 colors 37.574866 prices.condition 35.767574 merchants 28.133866 features 25.927288 prices.merchant 24.939582 manufacturerNumber 16.502049 imageURLs 6.320269 brand 3.325628 dtype: float64 . (miss_val[miss_val==0].index) . Index([&#39;prices.isSale&#39;, &#39;categories&#39;, &#39;dateAdded&#39;, &#39;dateUpdated&#39;, &#39;sourceURLs&#39;, &#39;prices.dateSeen&#39;, &#39;prices.sourceURLs&#39;, &#39;prices.dateAdded&#39;, &#39;keys&#39;, &#39;name&#39;, &#39;prices.amountMin&#39;, &#39;prices.amountMax&#39;, &#39;prices.currency&#39;, &#39;id&#39;], dtype=&#39;object&#39;) . WomenShoe = WomenShoe.drop(columns= [&#39;id&#39;, &#39;asins&#39;, &#39;count&#39;, &#39;descriptions&#39;, &#39;dimension&#39;,&#39;ean&#39;, &#39;flavors&#39;, &#39;isbn&#39;,&#39;manufacturer&#39;, &#39;manufacturerNumber&#39;, &#39;merchants&#39;, &#39;keys&#39;, &#39;imageURLs&#39;, &#39;prices.availability&#39;, &#39;prices.color&#39;, &#39;prices.condition&#39;, &#39;prices.count&#39;, &#39;prices.flavor&#39;, &#39;prices.merchant&#39;, &#39;prices.offer&#39;, &#39;prices.returnPolicy&#39;, &#39;prices.shipping&#39;, &#39;prices.size&#39;, &#39;prices.source&#39;, &#39;prices.warranty&#39;, &#39;quantities&#39;, &#39;reviews&#39;, &#39;skus&#39;, &#39;upc&#39;, &#39;websiteIDs&#39;, &#39;weight&#39;, &#39;prices.sourceURLs&#39;,&#39;prices.dateAdded&#39;, &#39;prices.dateSeen&#39;, &#39;dateUpdated&#39;, &#39;sizes&#39;, &#39;sourceURLs&#39;] ) #Check the new dimension of the dataset WomenShoe.shape . (19034, 10) . WomenShoe = WomenShoe.rename(columns={&quot;prices.amountMin&quot;: &quot;MinPrice&quot;, &quot;prices.amountMax&quot;: &quot;MaxPrice&quot;, &quot;prices.currency&quot;: &quot;currency&quot;}) WomenShoe.head(8) . brand categories colors dateAdded features name MinPrice MaxPrice currency prices.isSale . 0 | Zoot | Shoes,Clothing,Women&#39;s Shoes,All Women&#39;s Shoes | Blue,Multicolor | 2016-11-11 09:49:00+00:00 | [{&quot;key&quot;:&quot;Season&quot;,&quot;value&quot;:[&quot;All-Season&quot;]},{&quot;key... | Zoot Tt Trainer 2.0 Round Toe Synthetic Sne... | 71.99 | 71.99 | USD | True | . 1 | Zoot | Shoes,Clothing,Women&#39;s Shoes,All Women&#39;s Shoes | Blue,Multicolor | 2016-11-11 09:49:00+00:00 | [{&quot;key&quot;:&quot;Season&quot;,&quot;value&quot;:[&quot;All-Season&quot;]},{&quot;key... | Zoot Tt Trainer 2.0 Round Toe Synthetic Sne... | 250.00 | 250.00 | USD | False | . 2 | Zoot | Shoes,Clothing,Women&#39;s Shoes,All Women&#39;s Shoes | Blue,Multicolor | 2016-11-11 09:49:00+00:00 | [{&quot;key&quot;:&quot;Season&quot;,&quot;value&quot;:[&quot;All-Season&quot;]},{&quot;key... | Zoot Tt Trainer 2.0 Round Toe Synthetic Sne... | 75.99 | 75.99 | USD | True | . 3 | Zoot | Shoes,Clothing,Women&#39;s Shoes,All Women&#39;s Shoes | Blue,Multicolor | 2016-11-11 09:49:00+00:00 | [{&quot;key&quot;:&quot;Season&quot;,&quot;value&quot;:[&quot;All-Season&quot;]},{&quot;key... | Zoot Tt Trainer 2.0 Round Toe Synthetic Sne... | 79.98 | 79.98 | USD | True | . 4 | Wild Pair | Shoes,Women&#39;s Shoes,Clothing,All Women&#39;s Shoes | Brown | 2016-11-16 12:56:36+00:00 | [{&quot;key&quot;:&quot;Heel Height&quot;,&quot;value&quot;:[&quot;High (3 in. an... | Wild Pair Colfax Women Peep-toe Synthetic Bro... | 26.98 | 26.98 | USD | True | . 5 | Wild Pair | Shoes,Women&#39;s Shoes,Clothing,All Women&#39;s Shoes | Brown | 2016-11-16 12:56:36+00:00 | [{&quot;key&quot;:&quot;Heel Height&quot;,&quot;value&quot;:[&quot;High (3 in. an... | Wild Pair Colfax Women Peep-toe Synthetic Bro... | 15.99 | 15.99 | USD | True | . 6 | Wild Pair | Shoes,Women&#39;s Shoes,Clothing,All Women&#39;s Shoes | Brown | 2016-11-16 12:56:36+00:00 | [{&quot;key&quot;:&quot;Heel Height&quot;,&quot;value&quot;:[&quot;High (3 in. an... | Wild Pair Colfax Women Peep-toe Synthetic Bro... | 90.00 | 90.00 | USD | False | . 7 | Sutton Studio | Women&#39;s Suits &amp; Sets,Women&#39;s Clothing,All Wome... | Pink | 2016-01-02 03:16:24+00:00 | [{&quot;key&quot;:&quot;Style&quot;,&quot;value&quot;:[&quot;Suit jackets&quot;]},{&quot;ke... | Sutton Studio Women&#39;s 100 Cashmere Blazer Jacket | 318.00 | 318.00 | USD | False | . Correct Data Types . Actually, the data types for dateAdded, MinPrice, MaxPrice, and prices.isSale are all object when I first process this dataset. I tried to convert MinPrice and MaxPrice to float, but it didn&#39;t work. I looked at the data in Excel, I found there are so many rows with data that are overlapped with other columns, so I had to go over those 19,034 rows to find and correct the data manually. . WomenShoe.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 19034 entries, 0 to 19033 Data columns (total 10 columns): brand 18401 non-null object categories 19034 non-null object colors 11882 non-null object dateAdded 19034 non-null datetime64[ns, UTC] features 14099 non-null object name 19034 non-null object MinPrice 19034 non-null float64 MaxPrice 19034 non-null float64 currency 19034 non-null object prices.isSale 19034 non-null bool dtypes: bool(1), datetime64[ns, UTC](1), float64(2), object(6) memory usage: 1.3+ MB . Currency Change . WomenShoe[&#39;average_price&#39;] = (WomenShoe[&#39;MinPrice&#39;] + WomenShoe[&#39;MaxPrice&#39;])/2 #drop MinPrice and MaxPrice columns WomenShoe.drop(columns=[&#39;MinPrice&#39;, &#39;MaxPrice&#39;], axis=1, inplace=True) WomenShoe.head() #9 columns left . brand categories colors dateAdded features name currency prices.isSale average_price . 0 | Zoot | Shoes,Clothing,Women&#39;s Shoes,All Women&#39;s Shoes | Blue,Multicolor | 2016-11-11 09:49:00+00:00 | [{&quot;key&quot;:&quot;Season&quot;,&quot;value&quot;:[&quot;All-Season&quot;]},{&quot;key... | Zoot Tt Trainer 2.0 Round Toe Synthetic Sne... | USD | True | 71.99 | . 1 | Zoot | Shoes,Clothing,Women&#39;s Shoes,All Women&#39;s Shoes | Blue,Multicolor | 2016-11-11 09:49:00+00:00 | [{&quot;key&quot;:&quot;Season&quot;,&quot;value&quot;:[&quot;All-Season&quot;]},{&quot;key... | Zoot Tt Trainer 2.0 Round Toe Synthetic Sne... | USD | False | 250.00 | . 2 | Zoot | Shoes,Clothing,Women&#39;s Shoes,All Women&#39;s Shoes | Blue,Multicolor | 2016-11-11 09:49:00+00:00 | [{&quot;key&quot;:&quot;Season&quot;,&quot;value&quot;:[&quot;All-Season&quot;]},{&quot;key... | Zoot Tt Trainer 2.0 Round Toe Synthetic Sne... | USD | True | 75.99 | . 3 | Zoot | Shoes,Clothing,Women&#39;s Shoes,All Women&#39;s Shoes | Blue,Multicolor | 2016-11-11 09:49:00+00:00 | [{&quot;key&quot;:&quot;Season&quot;,&quot;value&quot;:[&quot;All-Season&quot;]},{&quot;key... | Zoot Tt Trainer 2.0 Round Toe Synthetic Sne... | USD | True | 79.98 | . 4 | Wild Pair | Shoes,Women&#39;s Shoes,Clothing,All Women&#39;s Shoes | Brown | 2016-11-16 12:56:36+00:00 | [{&quot;key&quot;:&quot;Heel Height&quot;,&quot;value&quot;:[&quot;High (3 in. an... | Wild Pair Colfax Women Peep-toe Synthetic Bro... | USD | True | 26.98 | . WomenShoe.currency.value_counts() . USD 18199 CAD 638 AUD 118 EUR 64 GBP 15 Name: currency, dtype: int64 . def curr_change(x): if x.currency == &#39;AUD&#39;: x.average_price = x.average_price * 0.73 if x.currency == &#39;CAD&#39;: x.average_price = x.average_price * 0.76 if x.currency == &#39;EUR&#39;: x.average_price = x.average_price * 1.19 if x.currency == &#39;GBP&#39;: x.average_price = x.average_price * 1.33 return x . WomenShoe[WomenShoe[&#39;currency&#39;] != &quot;USD&quot;].head() . brand categories colors dateAdded features name currency prices.isSale average_price . 42 | NaN | Shoes &amp; Handbags,Shoes,Women,Athletic | NaN | 2016-08-27 18:41:34+00:00 | NaN | Sneed- Customizable Women&#39;s Dance Shoes Latin/... | CAD | False | 88.00 | . 80 | DREAM GIRL | Clothing, Shoes &amp; Accessories,Costumes, Reenac... | Greek Roman | 2016-03-31 20:29:11+00:00 | [{&quot;key&quot;:&quot;Manufacturer Part Number&quot;,&quot;value&quot;:[&quot;5... | Adult Womens Sexy Greek Roman Goddess Toga Fan... | AUD | False | 39.99 | . 117 | NaN | Athletic,Women,Shoes,Shoes &amp; Handbags | White,Blue,Black,Burgundy | 2015-11-16 02:56:40+00:00 | NaN | Yu&amp;yu Women&#39;s Shoes Pointed Toe Chunky Heel Ox... | CAD | False | 154.22 | . 128 | NaN | Shoes &amp; Handbags,Shoes,Women,Athletic | NaN | 2016-08-27 18:41:27+00:00 | NaN | Sneed- Customizable Women&#39;s Dance Shoes Latin/... | CAD | False | 88.00 | . 142 | NaN | Athletic,Women,Shoes,Shoes &amp; Handbags | Blue,Red,Black | 2015-11-16 02:55:17+00:00 | NaN | Yu&amp;yu Women&#39;s Shoes Round Toe Flat Heel Flats ... | CAD | False | 57.32 | . WomenShoe = WomenShoe.apply(curr_change, axis=1) currency_change = WomenShoe[WomenShoe[&#39;currency&#39;] != &quot;USD&quot;] currency_change.head() . brand categories colors dateAdded features name currency prices.isSale average_price . 42 | NaN | Shoes &amp; Handbags,Shoes,Women,Athletic | NaN | 2016-08-27 18:41:34+00:00 | NaN | Sneed- Customizable Women&#39;s Dance Shoes Latin/... | CAD | False | 66.8800 | . 80 | DREAM GIRL | Clothing, Shoes &amp; Accessories,Costumes, Reenac... | Greek Roman | 2016-03-31 20:29:11+00:00 | [{&quot;key&quot;:&quot;Manufacturer Part Number&quot;,&quot;value&quot;:[&quot;5... | Adult Womens Sexy Greek Roman Goddess Toga Fan... | AUD | False | 29.1927 | . 117 | NaN | Athletic,Women,Shoes,Shoes &amp; Handbags | White,Blue,Black,Burgundy | 2015-11-16 02:56:40+00:00 | NaN | Yu&amp;yu Women&#39;s Shoes Pointed Toe Chunky Heel Ox... | CAD | False | 117.2072 | . 128 | NaN | Shoes &amp; Handbags,Shoes,Women,Athletic | NaN | 2016-08-27 18:41:27+00:00 | NaN | Sneed- Customizable Women&#39;s Dance Shoes Latin/... | CAD | False | 66.8800 | . 142 | NaN | Athletic,Women,Shoes,Shoes &amp; Handbags | Blue,Red,Black | 2015-11-16 02:55:17+00:00 | NaN | Yu&amp;yu Women&#39;s Shoes Round Toe Flat Heel Flats ... | CAD | False | 43.5632 | . Uniform Brand Names . WomenShoe.brand.value_counts() . Ralph Lauren 543 Nike 367 TOMS 327 MUK LUKS 237 Easy Spirit 232 ... Zerouv 1 301-42 SW-SM 1 Lee Rider 1 Marchon 1 WIG 1 Name: brand, Length: 2141, dtype: int64 . brand_map = {&#39;a2 by aerosoles&#39;:&#39;aerosoles&#39;, &quot;what&#39;s what by aerosoles&quot;:&#39;aerosoles&#39;, &#39;adidas outdoor&#39;:&#39;adidas&#39;, &#39;adriana new york&#39;:&#39;adriana&#39;, &#39;alexander mcqueen by puma&#39;:&#39;puma&#39;, &#39;alexander mcqueen&#39;:&#39;puma&#39;, &#39;alpine&#39;:&#39;alpine swiss&#39;, &#39;anne klein ak&#39;:&#39;anne klein&#39;, &#39;anne klein sport&#39;:&#39;anne klein&#39;, &#39;annie shoes&#39;:&#39;annie&#39;, &#39;athena&#39;:&#39;athena alexander&#39;, &#39;babe.&#39;:&#39;babe&#39;, &#39;baretraps&#39;:&#39;bare traps&#39;, &#39;bcbg max azria&#39;:&#39;bcbg&#39;, &#39;bcbg paris&#39;:&#39;bcbg&#39;, &#39;bcbgeneration&#39;:&#39;bcbg&#39;, &#39;beacon shoes&#39;:&#39;beacon&#39;, &#39;bebe sport&#39;:&#39;bebe&#39;, &#39;bebe stu&#39;:&#39;bebe&#39;, &#39;belle by sigerson morrison&#39;:&#39;belle&#39;, &#39;belle sigerson morrison&#39;:&#39;belle&#39;, &#39;bernie mev&#39;:&#39;bernie&#39;, &#39;bernie mev.&#39;:&#39;bernie&#39;, &#39;bettye by bettye muller&#39;:&#39;bettye muller&#39;, &#39;bettye by bettye muller &#39;:&#39;bettye muller&#39;, &quot;breckelle&#39;s&quot;:&#39;breckelles&#39;, &#39;callaway footwear&#39;:&#39;callaway&#39;, &#39;calvin klein ck&#39;:&#39;calvin klein&#39;, &#39;calvin klein jeans&#39;:&#39;calvin klein&#39;, &#39;carlos by carlos santana&#39;:&#39;carlos santana&#39;, &#39;charles by charles david&#39;:&#39;charels david&#39;, &#39;see by chloe&#39;:&#39;chloe&#39;, &#39;clarks artisan&#39;:&#39;clarks&#39;, &#39;clarks artisan collection&#39;:&#39;clarks&#39;, &#39;clarks collection&#39;:&#39;clarks&#39;, &#39;cobb hill by new balance&#39;:&#39;new balance&#39;, &#39;cobb hill&#39;:&#39;new balance&#39;, &#39;maria sharapova by cole haan&#39;: &#39;cole hann&#39;, &quot;corky&#39;s&quot;:&#39;corkys&#39;, &quot;corky&#39;s footwear&quot;:&#39;corkys&#39;, &quot;corkys footwear, inc.&quot;:&#39;corkys&#39;, &#39;dearforms&#39;:&#39;dearfoams&#39;, &#39;df by dearfoams&#39;:&#39;dearfoams&#39;, &#39;derek lam 10 crosby&#39;:&#39;derek lam&#39;, &#39;diba.true&#39;:&#39;dibatrue&#39;, &#39;dolce&amp;gabbana&#39;:&#39;dolce and gabbana&#39;, &#39;dolce &amp; gabbana&#39;:&#39;dolce and gabbana&#39;, &#39;dolce by mojo moxy&#39;:&#39;dolce and gabbana&#39;, &#39;dolce vita&#39;:&#39;dolce and gabbana&#39;, &#39;dv8 by dolce vita&#39;:&#39;dolce and gabbana&#39;, &#39;dv by dolce vita&#39;:&#39;dolce and gabbana&#39;, &quot;dr. scholl&#39;s&quot;:&#39;dr scholls&#39;, &quot;dr. martens air wair&quot;:&#39;drmartens&#39;, &#39;drew shoe&#39;:&#39;drew&#39;, &#39;easy spirit e360&#39;:&#39;easy spirit&#39;, &#39;easy spirit.&#39;:&#39;easy spirit&#39;, &#39;ellie shoes&#39;:&#39;ellie&#39;, &#39;emu australia&#39;:&#39;emu&#39;, &#39;fergie footwear&#39;:&#39;fergie&#39;, &#39;forever collectible&#39;:&#39;forever&#39;, &#39;forever link&#39;:&#39;forever&#39;, &#39;fourever funky&#39;:&#39;forever&#39;, &#39;sarto by franco sarto&#39;:&#39;franco sarto&#39;, &#39;ferriniusa&#39;:&#39;ferrini&#39;, &#39;fitflop&#39;:&#39;fit flop&#39;, &#39;funtasma by pleaser&#39;:&#39;funtasma&#39;, &#39;g by guess&#39;:&#39;guess&#39;, &#39;gc shoes&#39;:&#39;gc&#39;, &#39;genuine grip footwear&#39;:&#39;genuine grip&#39;, &quot;hogan by tod&#39;s&quot;:&#39;hogan&#39;, &#39;soft style by hush puppies&#39;:&#39;hush puppies&#39;, &#39;ilse jacobsen hornbaek&#39;:&#39;ilse jacobson&#39;, &#39;isaacmizrahi&#39;:&#39;isaac mizrahi&#39;, &#39;italian shoe makers&#39;:&#39;italian comfort&#39;, &#39;j.renee&#39;:&#39;j. renee&#39;, &#39;jbu by jambu&#39;:&#39;jambu&#39;, &#39;josefseibel&#39;:&#39;josef siebel&#39;, &#39;justin blair&#39;:&#39;justin&#39;, &#39;justin boots&#39;:&#39;justin&#39;, &#39;justin gypsy&#39;:&#39;justin&#39;, &#39;kate spade new york&#39;:&#39;kate spade&#39;, &#39;kenneth cole reaction&#39;:&#39;kenneth cole&#39;, &#39;kenneth cole ny&#39;:&#39;kenneth cole&#39;, &#39;kenneth cole new york&#39;:&#39;kenneth cole&#39;, &#39;unlisted kenneth cole&#39;:&#39;kenneth cole&#39;, &#39;lamo sheepskin inc&#39;:&#39;lamo&#39;, &#39;lifestride&#39;:&#39;lifes tride&#39;, &#39;luoluo&#39;:&#39;luo luo&#39;, &#39;marc fisher ltd&#39;:&#39;marc fisher&#39;, &#39;mia heritage&#39;:&#39;mia&#39;, &#39;micahel kors&#39;:&#39;michael kors&#39;, &#39;michael michael kors&#39;:&#39;michael kors&#39;, &#39;mobils by mephisto&#39;:&#39;mephisto&#39;, &#39;top moda&#39;:&#39;moda&#39;, &#39;moda essentials&#39;:&#39;moda&#39;, &#39;everybody by bz moda&#39;:&#39;moda&#39;, &#39;muk luks a la mode&#39;:&#39;muk luks&#39;, &#39;munro american&#39;:&#39;munro&#39;, &#39;naot footwear&#39;:&#39;naot&#39;, &#39;new@titude&#39;:&#39;new attitude&#39;, &#39;new@ttitude&#39;:&#39;new attitude&#39;, &#39;nina originals&#39;:&#39;nina&#39;, &#39;nine west vintage america collection&#39;:&#39;nine west&#39;, &#39;nufoot���&#39;:&#39;nufoot&#39;, &#39;pleaser shoes&#39;:&#39;pleaser&#39;, &#39;pleaser usa, inc.&#39;:&#39;pleaser&#39;, &#39;pleaserusa&#39;:&#39;pleaser&#39;, &#39;rachel&#39;:&#39;rachel roy&#39;, &#39;rachel rachel roy &#39;:&#39;rachel roy&#39;, &#39;lauren by ralph lauren&#39;:&#39;ralph lauren&#39;, &#39;lauren ralph lauren&#39;:&#39;ralph lauren&#39;, &#39;lauren lorraine&#39;:&#39;ralph lauren&#39;, &#39;polo ralph lauren&#39;:&#39;ralph lauren&#39;,&#39;ralph lauren denim supply&#39;:&#39;ralph lauren&#39;, &#39;rieker-antistress&#39;:&#39;rieker&#39;, &#39;rocket dog brands llc&#39;:&#39;rocket dog&#39;, &#39;sanita clogs&#39;:&#39;sanita&#39;, &#39;ferragamo&#39;:&#39;salvatore ferragamo&#39;, &#39;skechers usa&#39;:&#39;skechers&#39;, &#39;sperry top sider&#39;:&#39;sperry&#39;, &#39;sperry top-sider&#39;:&#39;sperry&#39;, &quot;l&#39;artiste by spring step&quot;:&#39;spring step&#39;, &quot;flexus by spring step&quot;:&#39;spring step&#39;, &quot;patrizia by spring step &quot;:&#39;spring step&#39;, &quot;patrizia pepe&quot;:&#39;spring step&#39;, &quot;patrizia&quot;:&#39;spring step&#39;, &#39;steven steve madden&#39;:&#39;steve madden&#39;, &#39;style &amp; co.&#39;:&#39;style and co&#39;, &#39;timberland earthkeepers&#39;:&#39;timberland&#39;, &#39;timberland pro&#39;:&#39;timberland&#39;, &#39;toms shoes&#39;:&#39;toms&#39;, &#39;tony lama boot co.&#39;:&#39;tony lama&#39;, &#39;totes isotoner&#39;:&#39;totes&#39;, &#39;trotter&#39;:&#39;trotters&#39;, &#39;ugg australia&#39;:&#39;ugg&#39;, &#39;famous name brand&#39;:&#39;unbranded&#39;, &#39;generic&#39;:&#39;unbranded&#39;, &#39;generic surplus&#39;:&#39;unbranded&#39;, &#39;non-branded&#39;:&#39;unbranded&#39;, &#39;not applicable&#39;:&#39;unbranded&#39;, &#39;not rated&#39;:&#39;unbranded&#39;, &#39;lucky brand&#39;:&#39;unbranded&#39;, &#39;lucky brand&#39;:&#39;unbranded&#39;, &#39;very fine dance shoes&#39;:&#39;unbranded&#39;, &#39;valentino noir&#39;:&#39;valentino&#39;, &#39;victoria k.&#39;:&#39;victoria&#39;, &#39;vince camuto&#39;:&#39;vince&#39;, &#39;vionic by orthaheel&#39;:&#39;vionic&#39;, &#39;vionic with orthaheel technology&#39;:&#39;vionic&#39;, &#39;elites by walking cradles&#39;:&#39;walking cradles&#39;, &#39;elites&#39;:&#39;walking cradles&#39;,&#39;mark lemp by walking cradles&#39;:&#39;walking cradles&#39;, &#39;rose petals by walking cradles&#39;:&#39;walking cradles&#39;, &#39;the walking cradle company&#39;:&#39;walking cradles&#39; } . Reference this note book https://www.kaggle.com/ashishg21/data-cleaning-and-some-analysis-shoe-prices. . WomenShoe[&#39;brand&#39;] = WomenShoe[&#39;brand&#39;].str.lower() # replace the brand names with the dictionary WomenShoe[&#39;brand_clean&#39;] = WomenShoe[&#39;brand&#39;].replace(brand_map) . WomenShoe.brand_clean.value_counts() . ralph lauren 606 nike 369 toms 328 easy spirit 270 muk luks 238 ... boss 1 lrl lauren jeans co. 1 mensusa.com 1 katuo 1 star bay 1 Name: brand_clean, Length: 1897, dtype: int64 . EDA . Color &amp; Price Analysis . What are the 10 most popular colors for women&#39;s shoe? . WomenShoe.colors.value_counts() . Black 1687 Brown 779 Beige 507 Blue 335 White 323 ... Mint,White,Nude,Neon Fuchsia 1 Powder Blue,Fig Purple,Chambray,Dusk 1 WhitePink 1 Black,Natural,Beige 1 Yellow,Beige,Orange 1 Name: colors, Length: 2473, dtype: int64 . color_rank = WomenShoe.colors.value_counts().head(10) # append 10 colors with the highest count in to a list rows = [] for i in range(10): rows.append([color_rank.index[i], color_rank[i]]) # convert the list to a pandas dataframe Col_Rank = pd.DataFrame(rows, columns=[&quot;color&quot;, &quot;count&quot;]) . plt.style.use(&#39;ggplot&#39;) fig, ax = plt.subplots(figsize=(12, 5)) Col_Rank = Col_Rank.sort_values(&#39;count&#39;) ax.barh(Col_Rank[&#39;color&#39;], Col_Rank[&#39;count&#39;], color= [&#39;pink&#39;, &#39;green&#39;, &#39;silver&#39;, &#39;red&#39;, &#39;gray&#39;, &#39;white&#39;, &#39;blue&#39;, &#39;beige&#39;, &#39;brown&#39;, &#39;black&#39;]) ax.set_ylabel(&quot;Color&quot;) ax.set_xlabel(&quot;Count&quot;) ax.set_title(&quot;Top 10 Color Count&quot;) plt.show() . Does the mediem price of each color also follow the color rank in the plot above? . Black = WomenShoe[WomenShoe[&#39;colors&#39;] == &quot;Black&quot;] Brown = WomenShoe[WomenShoe[&#39;colors&#39;] == &quot;Brown&quot;] Beige = WomenShoe[WomenShoe[&#39;colors&#39;] == &quot;Beige&quot;] Blue = WomenShoe[WomenShoe[&#39;colors&#39;] == &quot;Blue&quot;] White = WomenShoe[WomenShoe[&#39;colors&#39;] == &quot;White&quot;] Gray = WomenShoe[WomenShoe[&#39;colors&#39;] == &quot;Gray&quot;] # Make a copy of dataframes for each color Black1 = WomenShoe[WomenShoe[&#39;colors&#39;] == &quot;Black&quot;] Brown1 = WomenShoe[WomenShoe[&#39;colors&#39;] == &quot;Brown&quot;] Beige1 = WomenShoe[WomenShoe[&#39;colors&#39;] == &quot;Beige&quot;] Blue1 = WomenShoe[WomenShoe[&#39;colors&#39;] == &quot;Blue&quot;] White1 = WomenShoe[WomenShoe[&#39;colors&#39;] == &quot;White&quot;] Gray1 = WomenShoe[WomenShoe[&#39;colors&#39;] == &quot;Gray&quot;] . def outlier(x): y = (x[&#39;average_price&#39;].quantile(0.75) - x.average_price.quantile(0.25)) * 1.5 + x.average_price.quantile(0.75) return y print(outlier(Black)) print(outlier(Brown)) print(outlier(Beige)) print(outlier(Blue)) print(outlier(White)) print(outlier(Gray)) . 166.95499999999998 204.25250000000003 153.495 120.86250000000001 144.735 190.25250000000003 . Black.drop(Black[Black[&#39;average_price&#39;] &gt; 167].index, inplace = True) #Remove the outliers for Brown shoes Brown.drop(Brown[Brown[&#39;average_price&#39;] &gt; 205].index, inplace = True) #Remove the outliers for Beige shoes Beige.drop(Beige[Beige[&#39;average_price&#39;] &gt; 154].index, inplace = True) #Remove the outliers for Blue shoes Blue.drop(Blue[Blue[&#39;average_price&#39;] &gt; 121].index, inplace = True) #Remove the outliers for White shoes White.drop(White[White[&#39;average_price&#39;] &gt; 145].index, inplace = True) #Remove the outliers for Beige shoes Gray.drop(Gray[Gray[&#39;average_price&#39;] &gt; 191].index, inplace = True) . /Applications/Jupter/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py:4102: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy errors=errors, . plt.style.use(&#39;ggplot&#39;) fig, ax = plt.subplots(figsize=(15, 7)) ax.boxplot([Black[&quot;average_price&quot;], Brown[&quot;average_price&quot;], Beige[&quot;average_price&quot;], Blue[&quot;average_price&quot;], White[&quot;average_price&quot;], Gray[&quot;average_price&quot;]]) ax.set_xticklabels([&quot;Black&quot;, &quot;Brown&quot;, &quot;Beige&quot;, &quot;Blue&quot;, &quot;White&quot;, &quot;Gray&quot;]) ax.set_ylabel(&quot;Price&quot;) ax.set_xlabel(&quot;Color&quot;) ax.set_title(&#39;Box Plot for the Top 6 Colors&#39;) plt.show() . How prices are distributed in the top six colors? . plt.style.use(&#39;ggplot&#39;) fig, ax = plt.subplots(3, 2, figsize=(15, 8)) ax[0, 0].hist(Black1[&quot;average_price&quot;], bins=100, range=(0,500), color = &#39;black&#39;) ax[0,0].set_title(&#39;Black Shoes&#39;) ax[0,0].set_ylabel(&#39;Frequency&#39;) ax[0, 1].hist(Brown1[&quot;average_price&quot;], bins=100, range=(0,500), color = &#39;Brown&#39;) ax[0,1].set_title(&#39;Brown Shoes&#39;) ax[1, 0].hist(Beige1[&quot;average_price&quot;], bins=100, range=(0,500), color = &#39;Beige&#39;) ax[1,0].set_title(&#39;Beige Shoes&#39;) ax[1,0].set_ylabel(&#39;Frequency&#39;) ax[1, 1].hist(Blue1[&quot;average_price&quot;], bins=100, range=(0,500), color = &#39;Blue&#39;) ax[1,1].set_title(&#39;Blue Shoes&#39;) ax[2, 0].hist(White1[&quot;average_price&quot;], bins=100, range=(0,500), color = &#39;white&#39;) ax[2,0].set_title(&#39;White Shoes&#39;) ax[2,0].set_xlabel(&#39;Price&#39;) ax[2,0].set_ylabel(&#39;Frequency&#39;) ax[2, 1].hist(Gray1[&quot;average_price&quot;], bins=100, range=(0,500), color = &#39;gray&#39;) ax[2,1].set_title(&#39;Gray Shoes&#39;) ax[2,1].set_xlabel(&#39;Price&#39;) plt.tight_layout()#Get rid of overlaps plt.show() #chi-squared distribution . What is the average price for the top 10 colors in decending order? . y = WomenShoe.colors.value_counts().head(10) Colors = [] for i in range (10): Colors.append([y.index[i], y[i]]) # filter the origanial dataframe and keep all the rows that has those top ten colors dfc = [] for i in range (len(Colors)): dfc.append(Colors[i][0]) color_df = WomenShoe[WomenShoe[&#39;colors&#39;].isin(dfc)] . color_df.groupby(&#39;colors&#39;)[&#39;average_price&#39;].mean().sort_values(ascending=False) . colors Black 83.681328 White 78.665582 Blue 77.766768 Brown 77.556691 Gray 71.996876 Green 68.255517 Silver 65.643744 Multi-Color 62.177169 Red 60.700265 Beige 58.799527 Name: average_price, dtype: float64 . Brand &amp; Price Analysis . Which brand have the highest price? . x = WomenShoe.brand_clean.value_counts() brands = [] for i in range (1897): if x[i] &gt; 5: brands.append([x.index[i], x[i]]) #filter the origanial dataframe and keep all the brands that have at least 5 rows in the dataset df = [] for i in range (len(brands)): df.append(brands[i][0]) WS_Brand = WomenShoe[WomenShoe[&#39;brand_clean&#39;].isin(df)] #Calculate the average price for each brand and only keep the top 20 brands df_brand = WS_Brand.groupby(&#39;brand_clean&#39;)[&#39;average_price&#39;].mean().sort_values(ascending=False).head(20) Over5 = [] for i in range (len(df_brand)): Over5.append([df_brand.index[i], df_brand[i]]) Over5_df = pd.DataFrame(Over5, columns=[&quot;brand&quot;, &quot;average_price&quot;]) . plt.style.use(&#39;ggplot&#39;) fig, ax = plt.subplots(figsize=(12, 10)) Over5_df = Over5_df.sort_values(&#39;average_price&#39;) ax.barh(Over5_df[&#39;brand&#39;], Over5_df[&#39;average_price&#39;], color= &quot;blue&quot;) ax.set_ylabel(&quot;Brands&quot;) ax.set_xlabel(&quot;Average Price&quot;) ax.set_title(&quot;Top 20 brands with Count over 5&quot;) plt.show() . brands = [] for i in range (1897): if x[i] &gt; 50: brands.append([x.index[i], x[i]]) #filter the origanial dataframe and keep all the brands that have at least 50 rows in the dataset df = [] for i in range (len(brands)): df.append(brands[i][0]) WS_Brand = WomenShoe[WomenShoe[&#39;brand_clean&#39;].isin(df)] #Calculate the average price for each brand and only keep the top 20 brand df_brand = WS_Brand.groupby(&#39;brand_clean&#39;)[&#39;average_price&#39;].mean().sort_values(ascending=False).head(20) Over5 = [] for i in range (len(df_brand)): Over5.append([df_brand.index[i], df_brand[i]]) Over5_df = pd.DataFrame(Over5, columns=[&quot;brand&quot;, &quot;average_price&quot;]) #draw a bar chart for 20 brands with the highest average price plt.style.use(&#39;ggplot&#39;) my_cmap = plt.get_cmap(&quot;viridis&quot;) fig, ax = plt.subplots(figsize=(12, 10)) Over5_df = Over5_df.sort_values(&#39;average_price&#39;) ax.barh(Over5_df[&#39;brand&#39;], Over5_df[&#39;average_price&#39;], color= my_cmap.colors ) ax.set_ylabel(&quot;Brands&quot;) ax.set_xlabel(&quot;Average Price&quot;) ax.set_title(&quot;Top 20 brands with Count over 50&quot;) plt.show() . Specific Brand &amp; Price Analysis . Which ones have the widest distribution of prices? Is there a typical price distribution across brands or within specific brands? . plt.style.use(&#39;ggplot&#39;) fig, ax = plt.subplots() ax.hist(WomenShoe[&quot;average_price&quot;], bins=50, range=(0,800), color = &#39;orange&#39;) ax.set_title(&#39;Price Distribution Across Brands&#39;) ax.set_xlabel(&quot;Price&quot;) ax.set_ylabel(&quot;Frequency&quot;) plt.show() . Ralph_lauren = WomenShoe[WomenShoe[&#39;brand_clean&#39;] == &quot;ralph lauren&quot;] Ugg = WomenShoe[WomenShoe[&#39;brand_clean&#39;] == &quot;ugg&quot;] Nike = WomenShoe[WomenShoe[&#39;brand_clean&#39;] == &quot;nike&quot;] Puma = WomenShoe[WomenShoe[&#39;brand_clean&#39;] == &quot;puma&quot;] Toms = WomenShoe[WomenShoe[&#39;brand_clean&#39;] == &quot;toms&quot;] Vans = WomenShoe[WomenShoe[&#39;brand_clean&#39;] == &quot;vans&quot;] . plt.style.use(&#39;ggplot&#39;) fig, ax = plt.subplots(3, 2, figsize=(15, 8)) ax[0, 0].hist(Ralph_lauren[&quot;average_price&quot;], bins=50, range=(0,1500), color = &#39;cadetblue&#39;) ax[0,0].set_title(&#39;Ralph Lauren&#39;) ax[0,0].set_ylabel(&quot;Frequency&quot;) ax[0, 1].hist(Ugg[&quot;average_price&quot;], bins=50, range=(0,500), color = &#39;lightblue&#39;) ax[0,1].set_title(&#39;Ugg&#39;) ax[1, 0].hist(Nike[&quot;average_price&quot;], bins=50, range=(0,500), color = &#39;skyblue&#39;) ax[1,0].set_title(&#39;Nike&#39;) ax[1,0].set_ylabel(&quot;Frequency&quot;) ax[1, 1].hist(Puma[&quot;average_price&quot;], bins=50, range=(0,500), color = &#39;steelblue&#39;) ax[1,1].set_title(&#39;Puma&#39;) ax[2, 0].hist(Toms[&quot;average_price&quot;], bins=50, range=(0,500), color = &#39;dodgerblue&#39;) ax[2,0].set_title(&#39;Toms&#39;) ax[2,0].set_ylabel(&quot;Frequency&quot;) ax[2,0].set_xlabel(&quot;Average Price&quot;) ax[2, 1].hist(Vans[&quot;average_price&quot;], bins=50, range=(0,500), color = &#39;deepskyblue&#39;) ax[2,1].set_title(&#39;Vans&#39;) ax[2,1].set_xlabel(&quot;Average Price&quot;) plt.tight_layout()#Get rid of overlaps plt.show() . Ralph_lauren[&#39;average_price&#39;].plot(kind=&#39;density&#39;, color = &#39;cadetblue&#39;) plt.title(&#39;Price Distribution for Ralph Lauren&#39;) plt.xlabel(&#39;Price&#39;) plt.show() . Ugg[&#39;average_price&#39;].plot(kind=&#39;density&#39;, color = &#39;lightblue&#39;) plt.title(&#39;Price Distribution for Ugg&#39;) plt.xlabel(&#39;Price&#39;) plt.show() . Nike[&#39;average_price&#39;].plot(kind=&#39;density&#39;, color = &#39;skyblue&#39;) plt.title(&#39;Price Distribution for Nike&#39;) plt.xlabel(&#39;Price&#39;) plt.show() . Puma[&#39;average_price&#39;].plot(kind=&#39;density&#39;, color = &#39;steelblue&#39;) plt.title(&#39;Price Distribution for Puma&#39;) plt.xlabel(&#39;Price&#39;) plt.show() . Toms[&#39;average_price&#39;].plot(kind=&#39;density&#39;, color = &#39;dodgerblue&#39;) plt.title(&#39;Price Distribution for Toms&#39;) plt.xlabel(&#39;Price&#39;) plt.show() . Vans[&#39;average_price&#39;].plot(kind=&#39;density&#39;, color = &#39;deepskyblue&#39;) plt.title(&#39;Price Distribution for Vans&#39;) plt.xlabel(&#39;Price&#39;) plt.show() . Conclusion . Overall, I explored the price distribution for shoe colors and shoe brands. The price distribution for shoe colors are more like a chi-squared distribution. The price distribution for shoe brands can be close to normal distribution. .",
            "url": "https://shudi-zhao.github.io/My_Data_Science_Blog/eda/python/data%20vidualization/2020/12/01/Women-Shoes.html",
            "relUrl": "/eda/python/data%20vidualization/2020/12/01/Women-Shoes.html",
            "date": " • Dec 1, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "Shudi Zhao . Please visit my resumefor additional information! .",
          "url": "https://shudi-zhao.github.io/My_Data_Science_Blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://shudi-zhao.github.io/My_Data_Science_Blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}